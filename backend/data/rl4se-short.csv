project	key	title	abstract	doi	decision	mode	exclusion_criteria	reviewer_count
RL4SE	Abdelmoaty2022	Resilient Topology Design for Wireless Backhaul: A Deep Reinforcement Learning Approach	Ultra-dense 5G and beyond deployments are setting significant burden on cellular networks, especially for wireless backhauls. Today, a careful planning for wireless backhaul is more critical than ever. In this letter, we study the hierarchical wireless backhaul topology design problem. We introduce a Deep Reinforcement Learning (DRL) based algorithm that can solve the problem efficiently. We compare the quality of the solutions derived by our DRL approach to the optimal solution, derived according to the Integer Linear Program (ILP) formulation in our previous work. A simulation using practical channel propagation scenarios and different network densities proves that our DRL-based algorithm is providing a sub-optimal solution with different levels of resiliency. Our DRL algorithm is further shown to scale for larger instances of the problem.	https://dx.doi.org/10.1109/LWC.2022.3207358	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abdulrahman2022	Reinforcement-learning-based damping control scheme of a PV plant in wide-area measurement system		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135556791&doi=10.1007\%2fs00202-022-01615-3&partnerID=40&md5=2a201de2174f125867277d60badb5e45	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abichandani2022	Design and Implementation of Closed Loop Control of PSFB Topology Using Artificial Intelligence		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141558490&doi=10.4271\%2f2022-28-0121&partnerID=40&md5=2b8c8cfc78f84c5d35f722203ce067c8	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abichandani2021	Implementation of Decentralized Reinforcement Learning-Based Multi-Quadrotor Flocking	Enabling coordinated motion of multiple quadrotors is an active area of research in the field of small unmanned aerial vehicles (sUAVs). While there are many techniques found in the literature that address the problem, these studies are limited to simulation results and seldom account for wind disturbances. This paper presents the experimental validation of a decentralized planner based on multi-objective reinforcement learning (RL) that achieves waypoint-based flocking (separation, velocity alignment, and cohesion) for multiple quadrotors in the presence of wind gusts. The planner is learned using an object-focused, greatest mass, state-action-reward-state-action (OF-GM-SARSA) approach. The Dryden wind gust model is used to simulate wind gusts during hardware-in-the-loop (HWIL) tests. The hardware and software architecture developed for the multi-quadrotor flocking controller is described in detail. HWIL and outdoor flight tests results show that the trained RL planner can generalize the flocking behaviors learned in training to the real-world flight dynamics of the DJI M100 quadrotor in windy conditions.	https://dx.doi.org/10.1109/ACCESS.2021.3115711	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abirami2022	Crypto-Deep Reinforcement Learning Based Cloud Security For Trusted Communication	One of the most essential business models in modern information technology is cloud computing. It offers a variety of services for user interaction as well as low-cost hardware and software. Cloud services are built on newline virtualization designs that use multi-tenancy for improved resource management and newline strong isolation across several Virtual Machines (VMs). Despite advancements in virtualization, data security and isolation assurances continue to be the major difficulties faced by cloud providers using existing approaches. To overcome this problem, Deep Reinforcement Learning is applied to offload the task and also to detect the generalized attackers in the cloud network. This proposed solution enables remote data monitoring approaches such as identity-based linear classification algorithms for VM attack classification channels. It can minimise data secrecy and increase communication by using a reinforcement learning technique. The attacker channel is identified using identity-based linear classification when data is transferred/retrieved from the VM cloud server. When the classifier finds channel misbehaviour, the port or channel may be blocked, and the communication of other accessible ports may be modified maintaining the end to end communication secrecy using the improved Multi Agent Deep Reinforcement Learning (MADRL). The service verification is done to ensure that users have secure access to the cloud server. When an unknown request to the cloud server runs the key authentication to check the user authorization, this linear classification trains the existingside-channel attack datasets to the classifier and detects the VM cloud's attack channel. In terms of overall performance, the proposed methodology is investigated and compared to the existing approaches.	https://dx.doi.org/10.1109/ICSSIT53264.2022.9716429	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Abrazeh2023	Virtual Hardware-in-the-Loop FMU Co-Simulation Based Digital Twins for Heating, Ventilation, and Air-Conditioning (HVAC) Systems	In this paper, a novel self-adaptive control method based on a digital twin is developed and investigated for a multi-input multi-output (MIMO) nonlinear system, which is a heating, ventilation, and air-conditioning system. For this purpose, hardware-in-loop (HIL) and software-in-loop (SIL) are integrated to develop the digital twin control concept in a straightforward manner. A nonlinear integral backstepping (NIB) model-free control technique is integrated with the HIL (implemented as a physical controller) and SIL (implemented as a virtual controller) controllers to control the HVAC system without the need for dynamic feature identification. The main goal is to design the virtual controller to minimize the distinction between system outputs in the SIL and HIL setups. For this purpose, Deep Reinforcement Learning (DRL) is applied to update the NIB controller coefficients of the virtual controller based on the measured data of the physical controller. Since the temperature and humidity of HVAC systems should be regulated, the NIB controllers in the HIL and SIL are designed by the DRL algorithm in a multi-objective scheme (MO). In particular, the simulations of the HIL and SIL environments are coupled by a new advanced tool: function mockup interface (FMI) standard. The Functional Mock-up Unit (FMU) is adopted into the FMI interface for data exchange. The extensive research of HIL and SIL controllers shows that the system outputs of the virtual controller are controlled exactly according to the physical controller.	https://dx.doi.org/10.1109/TETCI.2022.3168507	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Achamrah2022	Bi-level programming for modeling inventory sharing in decentralized supply chains		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127480149&doi=10.1016\%2fj.trpro.2022.02.064&partnerID=40&md5=abb94485282c37def68c3e904694b8de	Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Adel2021	A Multi-agent Reinforcement Learning Risk Management Model for Distributed Agile Software Projects	Nowadays, due to the benefits of the agile method, such as changing rapidly and fast delivery of working software, most software projects are naturally distributed agile projects (DAD). DAD team members work from various remote sites. This leads to the emergence of many significant challenges in risk management. Hence, it requires risk safety techniques to be implemented to mitigate the occurrence of risks. There is no standardized process for teams to deal with DAD risks. In this paper, a multi-agent reinforcement learning risk management model for distributed agile software projects is proposed. The proposed model is implemented to apply a dynamic policy. The model is applied as an experiment to definite numbers for a set of risk factors such as communication and coordination risk, project management risk, and SDLC risk. The proposed model is developed using multiple individual learning using the Q- learning algorithm. A DAD project with two projects is used to evaluate the proposed model. The proposed model was assessed and analyzed for its effectiveness. The results of the evaluation are given.	https://dx.doi.org/10.1109/ICICIS52592.2021.9694252	Included	new_screen		6
RL4SE	Adriaenssens2022	Learning to Ascend Stairs and Ramps: Deep Reinforcement Learning for a Physics-Based Human Musculoskeletal Model	This paper proposes to use deep reinforcement learning to teach a physics-based human musculoskeletal model to ascend stairs and ramps. The deep reinforcement learning architecture employs the proximal policy optimization algorithm combined with imitation learning and is trained with experimental data of a public dataset. The human model is developed in the open-source simulation software OpenSim, together with two objects (i.e., the stairs and ramp) and the elastic foundation contact dynamics. The model can learn to ascend stairs and ramps with muscle forces comparable to healthy subjects and with a forward dynamics comparable to the experimental training data, achieving an average correlation of 0.82 during stair ascent and of 0.58 during ramp ascent across both the knee and ankle joints.	https://www.ncbi.nlm.nih.gov/pubmed/36366177	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aghli2018	A reinforcement learning approach to autonomous speed control in robotic systems			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Agrawal2021	A multi-agent reinforcement learning framework for intelligent manufacturing with autonomous mobile robots		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117824775&doi=10.1017\%2fpds.2021.17&partnerID=40&md5=158b6e89fc56ce84df98a40148f321f1	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aguirre2021	A pre-expectation calculus for probabilistic sensitivity		https://doi.org/10.1145/3434333	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aguzzi2022	Addressing Collective Computations Efficiency: Towards a Platform-level Reinforcement Learning Approach	Aggregate Computing is a macro-level approach for programming collective intelligence and self-organisation in distributed systems. In this paradigm, system behaviour unfolds as a combination of a system-wide program, functionally manipulating distributed data structures called computational fields, and a distributed protocol where devices work at asynchronous rounds comprising sense-compute-interact steps. Interestingly, there exists a large amount of flexibility in how aggregate systems could actually execute while preserving the desired functionality. The ideal place for making choices about execution is the aggregate computing platform (or middleware), which can be engineered with the goal of promoting efficiency and other non-functional goals. In this work, we explore the possibility of applying Reinforcement Learning at the platform level in order to optimise aspects of a collective computation while achieving coherent functional goals. This idea is substantiated through synthetic experiments of data propagation and collection, where we show how Q-Learning could reduce the power consumption of aggregate computations.	https://dx.doi.org/10.1109/ACSOS55765.2022.00019	Included	conflict_resolution		6
RL4SE	Ahmad2020	Using Deep Reinforcement Learning for Exploratory Performance Testing of Software Systems With Multi-Dimensional Input Spaces	During exploratory performance testing, software testers evaluate the performance of a software system with different input combinations in order to identify combinations that cause performance problems in the system under test. Performance problems such as low throughput, high response times, hangs, or crashes in software applications have an adverse effect on the customer's satisfaction. Since many of today's large-scale, complex software systems (e.g., eCommerce applications, databases, web servers) exhibit very large multi-dimensional input spaces with many input parameters and large ranges, it has become costly and inefficient to explore all possible combinations of inputs in order to detect performance problems. In order to address this issue, we introduce a method for identifying input combinations that trigger performance problems in the software system under test. Our method, under the name of iPerfXRL, employs deep reinforcement learning in order to explore a given large multi-dimensional input space efficiently. The main benefit of the approach is that, during the exploration process, it learns and recognizes the problematic regions of the input space that have a higher chance of triggering performance problems. It concentrates the search in those problematic regions to find as many input combinations as possible that can trigger performance problems while executing a limited number of input combinations against the system. In addition, our approach does not require prior domain knowledge or access to the source code of the system. Therefore, it can be applied to any software system where we can interactively execute different input combinations while monitoring their performance impact on the system. We implement iPerfXRL on top of the Soft Actor-Critic algorithm. We evaluate empirically the efficiency and effectiveness of our approach against alternative state-of-the-art approaches. Our results show that iPerfXRL accurately identifies the problematic regions of the input space and finds up to 9 times more input combinations that trigger performance problems on the system under test than the alternative approaches.	https://dx.doi.org/10.1109/ACCESS.2020.3033888	Included	new_screen		6
RL4SE	Ahmad2019	Exploratory Performance Testing Using Reinforcement Learning	Performance bottlenecks resulting in high response times and low throughput of software systems can ruin the reputation of the companies that rely on them. Almost two-thirds of performance bottlenecks are triggered on specific input values. However, finding the input values for performance test cases that can identify performance bottlenecks in a large-scale complex system within a reasonable amount of time is a cumbersome, cost-intensive, and time-consuming task. The reason is that there can be numerous combinations of test input values to explore in a limited amount of time. This paper presents PerfXRL, a novel approach for finding those combinations of input values that can reveal performance bottlenecks in the system under test. Our approach uses reinforcement learning to explore a large input space comprising combinations of input values and to learn to focus on those areas of the input space which trigger performance bottlenecks. The experimental results show that PerfxRL can detect 72\% more performance bottlenecks than random testing by only exploring the 25\% of the input space.	https://dx.doi.org/10.1109/SEAA.2019.00032	Included	new_screen		6
RL4SE	Ahmadi2022	A DQN-based agent for automatic software refactoring	Context: Nowadays, technical debt has become a very important issue in software project management. The main mechanism to repay this debt is through refactoring. Refactoring software projects usually comes at a high cost. As a result, researchers have always looked for ways to minimize this cost, and a good potential candidate to reduce the cost of a process is to automate it. Objective: One of the automatic software refactoring methods that recently has received a lot of attention is based on search-based software engineering (SBSE) methods. Although because of comprehensiveness and versatility SBSE is considered an appropriate method for automatic refactoring, it has its downsides, the most important of which are the uncertainty of the results and the exponential execution time. Method: In this research, a solution is proposed inspired by search-based refactoring while taking advantage of exploitation in reinforcement learning techniques. This work aims to solve the uncertainty problems and execution time for large programs. In the proposed approach, the problem of uncertainty is solved by targeting the selection of refactoring actions used in the search-based approach. Also, due to the reduction of the dependency between the choice of the appropriate refactoring and its execution time, the time problem in large software refactoring has been greatly improved. Results: Amongst the performed evaluations and specifically for the refactoring of the largest case study, the proposed approach managed to increase the accuracy to more than twice of the SBSE refactoring approaches, while reducing the execution time of refactoring by more than 98\%. Conclusion: The results of the tests show that with increasing the volume and size of the software, the performance of the proposed approach also improves compared to the methods based on SBSE, both in terms of reducing technical debt and speeding up the refactoring process.	https://dx.doi.org/10.1016/j.infsof.2022.106893	Included	new_screen		6
RL4SE	Ahmed2020	An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications		https://doi.org/10.1007/s00500-020-04769-z	Included	new_screen		6
RL4SE	Ahmed2015	Memory modelling schemes in neuromorphic VLSI chips using reinforcement learning based on cognition a computational cognititve neuroscience approach			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ahmed2017	Building load management clusters using reinforcement learning	In this paper we introduce a customer selection model for decision making in a Demand response program. In particular, we focus on modelling demand response as a reinforcement learning problem that decomposes the customers into clusters based on their ability to provide curtailments at time of Demand response signal. The reinforcement learning approach allows the retailer to make fast informed decision on the customers reliable to provide demand management capabilities without the need of exploring the entire set of customers when needed in real-time and allow for classification of future customers in appropriate clusters. We demonstrate using this approach on a representative example to create clusters based on provided customer profiles for varying DR signals.	https://dx.doi.org/10.1109/IEMCON.2017.8117147	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ahrarinouri2021	Multiagent Reinforcement Learning for Energy Management in Residential Buildings		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096037876&doi=10.1109\%2fTII.2020.2977104&partnerID=40&md5=22e7ee052871d2f4b8c4ff135bad87e9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ahuja2021	Learning to Optimize Molecular Geometries Using Reinforcement Learning	Though quasi-Newton methods have been widely adopted in computational chemistry software for molecular geometry optimization, it is well known that these methods might not perform well for initial guess geometries far away from the local minima, where the quadratic approximation might be inaccurate. We propose a reinforcement learning approach to develop a model that produces a correction term for the quasi-Newton step calculated with the BFGS algorithm to improve the overall optimization performance. Our model is able to complete the optimization in about 30\% fewer steps than pure BFGS for molecules starting from perturbed geometries. The new method has similar convergence to BFGS when complemented with a line search procedure, but it is much faster since it avoids the multiple gradient evaluations associated with line searches.	https://www.ncbi.nlm.nih.gov/pubmed/33470813	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aiba1996	A computational model for distributed knowledge systems with learning mechanisms	This paper addresses the issues of machine learning in distributed knowledge systems, which will consist of distributed software agents with problem solving, communication and learning functions. To develop such systems, we must analyze the roles of problem-solving and communication capabilities among knowledge systems. To facilitate the analyses, we propose a computational model: LPC. The model consists of a set of agents with (a) a knowledge base for learned concepts, (b) a knowledge base for problem solving, (c) prolog-based inference mechanisms and (d) a set of beliefs on the reliability of the other agents. Each agent can improve its own problem-solving capabilities by deductive learning from the given problems, by memory-based learning from communications between the agents and by reinforcement learning from the reliability of communications between the other agents. An experimental system of the model has been implemented in prolog language on a Window-based personal computer Intensive experiments have been carried out to examine the feasibility of the machine learning mechanisms of agents for problem-solving and communication capabilities. The experimental results have shown that the multiagent system improves the performance of the whole system in problem solving, when each agent has a higher learning ability or when an agent with a very high ability for problem solving joins the organization to cooperate with the other agents in problem solving. These results suggest that the proposed model is useful in analyzing the learning mechanisms applicable to distributed knowledge systems. Copyright (C) 1996 Elsevier Science Ltd	https://dx.doi.org/10.1016/0957-4174(96)00020-6	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aissani2012	Dynamic scheduling for multi-site companies: a decisional approach based on reinforcement multi-agent learning		https://doi.org/10.1007/s10845-011-0580-y	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Akbarinasaji2018	Prioritizing lingering bugs		https://doi.org/10.1145/3178315.3178326	Included	new_screen		6
RL4SE	Akther2022	Interest forwarding strategy in Named Data Networks (NDN) using Thompson Sampling	Optimizing Interest forwarding and Data delivery has been among the top dissected problems in NDN for the last decade; however, only a few contributions thrive to minimize communication cost and delay concurrently. In NDN, a receiver-driven forwarding strategy is considered resource-consuming as the routers incur computation to find the best path to the desired item, specified by an Interest's name. On the other hand, a source-driven forwarding strategy, a scheme that suppresses the sub-optimal sources, experiences increased delay when no source answers in the exploration phase. The confluence of the two strategies can counteract the drawbacks of each one, which, however, has never been investigated. In this work, a reinforcement learning -based, namely Thompson Sampling, strategy is proposed that operates in a receiver-cum-source-driven fashion to optimize Interest forwarding and answering. The proposed method introduces a 'Beam' concept coupled with adaptive scoped-flooding to optimize Interest forwarding, and the sources adopt Thompson Sampling to suppress the sub-optimal responses. When hit by an Interest, an optimal source sends back the desired Data to the consumer whereas a sub-optimal source remains Silent. Together, the 'Beam' and the scoped-flooding adapt the Interest forwarding range based on cache hit/miss ratio. The adaptation optimizes communication cost and delay, and contributes to scheming the proposed strategy resource-savvy. The proof-of-concept implementation in software (simulation) reveals that the proposed system outperforms the counterpart benchmarks by reducing the communication costs and delay in NDN (by around 350\% and 10\%, respectively) without negotiating packet delivery ratio.	https://dx.doi.org/10.1016/j.jnca.2022.103458	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alawsi2022	Quality of service system that is self-updating by intrusion detection systems using reinforcement learning	Machine learning techniques are widely used in many areas and have also been used in intrusion detection systems (IDS) to identify hard-to-recognize patterns that can distinguish benign traffic from attack. However, these types of relationships and patterns are discovered through training, and there are several types of methods used to train. First, supervised methods, through their application to samples of benign and offensive traffic, secondly, unsupervised methods. Either way, human interaction is necessary to update the IDS, so that a new type of attack cannot be accepted in the protected network. In this study, a new method based on reinforcement learning is proposed. The proposed method has the potential to adapt to new types of attacks based on the quality of service of the network. The proposed method in this paper showed the best performance in terms of filtering the attack traffic that was not included in the training of the neural network used by the agent to choose the appropriate action for each packet. The program was evaluated using the CICIDS2017 dataset and achieved a F1 score of 0.96, compared to only 0.51 achieved using a classification-based deep learning approach.	https://dx.doi.org/10.1007/s13204-021-02172-0	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albahli2019	A Deep Ensemble Learning Method for Effort-Aware Just-In-Time Defect Prediction	Since the introduction of just-in-time effort aware defect prediction, many researchers are focusing on evaluating the different learning methods, which can predict the defect inducing changes in a software product. In order to predict these changes, it is important for a learning model to consider the nature of the dataset, its unbalancing properties and the correlation between different attributes. In this paper, we evaluated the importance of these properties for a specific dataset and proposed a novel methodology for learning the effort aware just-in-time prediction of defect inducing changes. Moreover, we devised an ensemble classifier, which fuses the output of three individual classifiers (Random forest, XGBoost, Multi-layer perceptron) to build an efficient state-of-the-art prediction model. The experimental analysis of the proposed methodology showed significant performance with 77\% accuracy on the sample dataset and 81\% accuracy on different datasets. Furthermore, we proposed a highly competent reinforcement learning technique to avoid false alarms in real time predictions.	https://dx.doi.org/10.3390/fi11120246	Included	new_screen		6
RL4SE	Albeaik2019	Deep Truck : A deep neural network model for longitudinal dynamics of heavy duty trucks	This article demonstrates the use of deep neural networks (NN) and deep reinforcement learning (deep-RL) for modeling and control of longitudinal heavy duty truck dynamics. Instead of explicit use of analytical model derived information or parameters about the truck, the deep NN model is fitted to data using a brief set of historical data collected from an arbitrary driving cycle. The deep model is used in this article to design a cruise controller for the truck using model-free deep-RL. The deep model and the control loop performances are demonstrated both using state-of-the-art commercial simulation software, and using a real-physical truck. Model and control performances are compared to classical physics-based modeling and control design approaches. The deep NN model is shown to capture latent nonlinear state dynamics and the deep-RL cruise controller is shown to achieve comparable results to a carefully designed and calibrated controller.	https://dx.doi.org/10.1109/ITSC.2019.8917038	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albeaik2019a	Deep Truck	This article demonstrates the use of deep neural networks (NN) and deep reinforcement learning (deep-RL) for modeling and control of longitudinal heavy duty truck dynamics. Instead of explicit use of analytical model derived information or parameters about the truck, the deep NN model is fitted to data using a brief set of historical data collected from an arbitrary driving cycle. The deep model is used in this article to design a cruise controller for the truck using model-free deep-RL. The deep model and the control loop performances are demonstrated both using state-of-the-art commercial simulation software, and using a real-physical truck. Model and control performances are compared to classical physics-based modeling and control design approaches. The deep NN model is shown to capture latent nonlinear state dynamics and the deep-RL cruise controller is shown to achieve comparable results to a carefully designed and calibrated controller.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albeaik2019b	Deep Truck : A deep neural network model for longitudinal dynamics of heavy duty trucks		https://doi.org/10.1109/ITSC.2019.8917038	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Albericci2021	A curriculum-based reinforcement learninig approach to pedestrian simulation			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Dayaa2012	Towards a Multiple-Lookahead-Levels agent reinforcement-learning technique and its implementation in integrated circuits		https://doi.org/10.1007/s11227-011-0738-6	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alfaverh2020	Demand Response Strategy Based on Reinforcement Learning and Fuzzy Reasoning for Home Energy Management		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081669247&doi=10.1109\%2fACCESS.2020.2974286&partnerID=40&md5=7aee2012044e2087f1eefb60b9864780	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Gabalawy2019	Machine learning for aircraft control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Hefnawi2021	Reinforcement Learning Method for Autonomous UAVs Monitoring an Uncertain Target	Autonomous unmanned aerial vehicles are able to sense their surrounding environments, and fly safely with little or no human intervention. Autonomous unmanned aerial vehicles are characterized by their ability to make decisions based on predicting future possible situations and learning from previous experiences. In this paper, we aim at developing algorithms that enable unmanned aerial vehicles to monitor and detect a dynamic uncertain target autonomously. This work considers a real monitoring system consists of a mission area, an autonomous unmanned aerial vehicle, a charging station, and a dynamic uncertain target. The mission area consists of two main areas, which are the area where the charging station is placed and the area where the target moves. The target area is divided to a number of subareas. We also adopt a time slotted system that has M equal-duration slots. The unmanned aerial vehicle is equipped with a battery of finite energy that can be recharged from the charging station. It can fly from one subarea to another during one time slot. The target moves from one subarea to another according to an unknown Markov process. In this context, we propose to using reinforcement learning algorithms that enables autonomous unmanned aerial vehicles to learn the movement of a dynamic uncertain target autonomously. Simulation results show that reinforcement learning algorithms outperform the performance of random and circular algorithms.11This work was supported by the ASPIRE Award for Research Excellence Program 2020 (Abu Dhabi, UAE) under grant AARE20-161.	https://dx.doi.org/10.1109/SNAMS53716.2021.9732147	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Al-Hilo2022	Reconfigurable Intelligent Surface Enabled Vehicular Communication: Joint User Scheduling and Passive Beamforming	Given its ability to control and manipulate wireless environments, reconfigurable intelligent surface (RIS), also known as intelligent reflecting surface (IRS), has emerged as a key enabler technology for the six-generation (6G) cellular networks. In the meantime, vehicular environment radio propagation is negatively influenced by a large set of objects that cause transmission distortion such as high buildings. Therefore, this work is devoted to explore the area of RIS technology integration with vehicular communications while considering the dynamic nature of such communication environment. Specifically, we provide a system model where RoadSide Unit (RSU) leverages RIS to provide indirect wireless transmissions to disconnected areas, known as dark zones. Dark zones are spots within RSU coverage where the communication links are blocked due to the existence of blockages. In details, a discrete RIS is utilized to provide communication links between the RSU and the vehicles passing through out-of-service zones. Therefore, the joint problem of RSU resource scheduling and RIS passive beamforming or phase-shift matrix is formulated as an optimization problem with the objective of maximizing the minimum average bit rate. The formulated problem is mixed integer non-convex program which is difficult to be solved and does not account for the uncertain dynamic environment in vehicular networks. Thereby, we resort to alternative methods based on Deep Reinforcement Learning to determine RSU wireless scheduling and Block Coordinate Descent (BCD) to solve for the phase-shift matrix, i.e., passive beamforming, of the RIS. The Markov Decision Process (MDP) is defined and the complexity of the solution approach is discussed. Our numerical results demonstrate the superiority of our proposed approach over baseline techniques.	https://dx.doi.org/10.1109/TVT.2022.3141935	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alhussein2022	Dynamic Topology Design of NFV-Enabled Services Using Deep Reinforcement Learning	Next-generation networks are endowed with enhanced capabilities thanks to software-defined networking and network function virtualization (NFV). There is a radical shift from device-centric to experience-driven environments of which data is the primary driver behind its running engines. In this paper, we consider joint topology design, traffic routing and NF placement for unicast NFV-enabled services. We develop an end-to-end model-free deep reinforcement learning (RL) framework to dynamically allocate processing and transmission resources, while considering time-varying network traffic patterns. First, we provide a flexible pre-processing technique that represents and reduces the state space and action space of the considered joint problem for the deep RL algorithm. Second, we present a deep deterministic policy gradient (DDPG) algorithm that is enhanced with a model-assisted exploration procedure. Due to the multiple resource types with strongly adverse effects, the existing vanilla DDPG algorithm cannot achieve consistent performance. The model-assisted exploration procedure, which utilizes a perturbed step-wise sub-optimal integer linear program, bootstraps and stabilizes the vanilla DDPG algorithm and finds optimal solutions efficiently.	https://dx.doi.org/10.1109/TCCN.2021.3139632	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alonso2003	Agency, learning and animal-based reinforcement learning		https://doi.org/10.1007/978-3-540-25928-2_1	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported	6
RL4SE	Alonso-Lupez2022	Level of Trust and Privacy Management in 6G Intent-based Networks for Vertical Scenarios	Security is one of the main concerns when designing future 6G networks. Most of the components of service chains are software-based and their security must be ensured. In this work we propose a system that will check the level of trust of 6G services following 3 complementary approaches: attestation, generation of proofs of transit and usage of smart contracts whose execution will be published via Distributed Ledger Technology in order to enable their traceability. The service orchestration function will use reinforcement learning to formulate optimised service orchestration decisions. Services will be specified in a declarative model, following a privacy-aware flavour of intent-based networking. The proposed architecture will take into account privacy of every stakeholder: final users, service providers and infrastructure providers.	https://dx.doi.org/10.1109/6GNet54646.2022.9830323	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper	6
RL4SE	Alroobaea2022	Markov decision process with deep reinforcement learning for robotics data offloading in cloud network		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147505356&doi=10.1117\%2f1.JEI.31.6.061809&partnerID=40&md5=a36f8a9e7dcc6742b5a7049503bb5995	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Alrubyli2022	Using Q-learning to Automatically Tune Quadcopter PID Controller Online for Fast Altitude Stabilization	Unmanned Arial Vehicles (UAVs), and more specifically, quadcopters need to be stable during their flights. Altitude stability is usually achieved by using a PID controller that is built into the flight controller software. Furthermore, the PID controller has gains that need to be tuned to reach optimal altitude stabilization during the quadcopter's flight. For that, control system engineers need to tune those gains by using extensive modeling of the environment, which might change from one environment and condition to another. As quadcopters penetrate more sectors from the military to the consumer sectors, they have been put into complex and challenging environments more than ever before. Hence, intelligent self-stabilizing quadcopters are needed to maneuver through those complex environments and situations. Here we show that by using online reinforcement learning with minimal background knowledge, the altitude stability of the quadcopter can be achieved using a model-free approach. We found that by using background knowledge and an activation function like Sigmoid, altitude stabilization can be achieved faster with a small memory footprint. In addition, using this approach will accelerate development by avoiding extensive simulations before applying the PID gains to the real-world quadcopter. Our results demonstrate the possibility of using the trial and error approach of reinforcement learning combined with activation function and background knowledge to achieve faster quadcopter altitude stabilization in different environments and conditions.	https://dx.doi.org/10.1109/ICMA54519.2022.9856292	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	AL-sarray2020	Securing 5G Network using low power wireless personal area network	The purpose of this work was to get acquainted with wireless communication technologies and the information security challenges they create from the viewpoint of 5G and its preceding technologies. 5th Generation (5G) is becoming a global phenomenon and it is currently being implemented in dozens of countries around the globe with it comes new information security challenges. Potential solutions for the challenges are also offered. The outcome of this research is an overview of information security challenges in 5G using Low-power wireless personal area Network (LPWAN) and in the technologies preceding 5G. Possible information security solutions are presented in this work for the new technologies coming with 5G. This work showed that the new technologies coming with 5G, such as the virtualization of hardware and services as well as the utilization of cloud computing, create completely new areas of attack for networks. With this knowledge, Labelled and Freely Available Dataset from Open-Source Repository will be used and it is possible to prevent attacks targeting networks by implementing necessary information security elements. For the training, testing and validation of our dataset which is an IoT and cyber-security based dataset, a well-known MATLAB R2019a software was used for this purpose. The proposed reinforcement learning algorithm for Securing 5G network is designed for mesh topology from the ground up by the model of the network itself using low power personal area networks. We model the network operating in a finite area with a finite number of nodes distributed inside the area randomly in this algorithm. Hence, we defined the service area of the target network by assuming the finiteness of the network in the model.	https://dx.doi.org/10.1109/ISMSIT50672.2020.9255054	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	6
RL4SE	Altin2020	Evolutionary Reinforcement Learning for the Coordination of Swarm UAVs	Deep Reinforcement Learning (DRL) algorithms are used in many challenging tasks and their usage areas are rapidly increasing. One of these areas is the formation flights of Unmanned Aerial Vehicles (UAVs). The rising of Reinforcement Learning (RL) algorithms performances is directly proportional to the development of environments. This paper presents a new environment developed through software (Ardupilot, Mavlink, drone-kit) that is frequently used in open source UAV simulation and programming, and the performance of the Evolutionary Reinforcement Learning (ERL) agent in this environment. The difference of this environment is that, unlike other environments, the model can be operated directly on a drone-kit supported vehicle and is specifically defined on the centralised formation task. The aim of this study is; in order to question the performance of the Evolutionary Reinforcement Learning (ERL) algorithm which has better results than other algorithms in DRL training environments,in this environment, and increasing the usage of the algorithm in this direction.	https://dx.doi.org/10.1109/SIU49456.2020.9302227	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Amaral2022	Deep Reinforcement Learning Based Routing in IP Media Broadcast Networks: Feasibility and Performance	The media broadcast industry has evolved from Serial Digital Interface (SDI) based infrastructures to IP networks. While IP based video broadcast is well established in the data plane, the use of IP networks to transport media flows still poses challenges in terms of resource management and orchestration. Software Defined Networking (SDN) based orchestration architectures have emerged in the industry that use SDN to route the media flows of a broadcast service across the provider IP network. Several approaches to multimedia flow routing in IP based SDN networks have been proposed in the context of streaming applications over the Internet. These range from model based linear optimization solutions that have high complexity to simple shortest path based routing with either Static Link Costs (SLC) or Dynamic Link Costs (DLC). More recently model-free optimization methods such as Deep Reinforcement Learning (DRL) have been proposed for routing and Traffic Engineering (TE) of multimedia flows in SDN networks. The media broadcast scenario however has specific requirements, with services like Master Control Room (MCR) operation and live broadcasting of events, and it has been rarely addressed in the literature. In this work we propose a DRL based routing method for this scenario and compare it to SLC and DLC algorithms based on Dijkstra shortest paths. This is, to our knowledge, the first work to follow this approach in the context of media broadcast services in IP infrastructures. The algorithm is designed considering the specifications and capabilities of one of the leading SDN orchestrators in the market and considers the more common Service Level Agreement (SLA) requirements in the industry. Three different DRL algorithms are implemented and compared and we evaluate them using a real service provider network topology. The results indicate that DRL based routing is applicable in real production scenarios and that it achieves considerable performance gains when compared to the SLC and DLC shortest path algorithms commonly used today.	https://dx.doi.org/10.1109/ACCESS.2022.3182009	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Amoui2008	Adaptive Action Selection in Autonomic Software Using Reinforcement Learning	"The planning process in autonomic software aims at selecting an action from a finite set of alternatives for adaptation. This is an abstruse problem due to the fact that software behaviour is usually very complex with numerous number of control variables. This research work focuses on proposing a planning process and specifically an action selection technique based on ""Reinforcement Learning"" (RL). We argue why, how, and when RL can be beneficial for an autonomic software system. The proposed approach is applied to a simulated model of a news web application. Evaluation results show that this approach can learn to select appropriate actions in a highly dynamic environment. Furthermore, we compare this approach with another technique from the literature, and the results suggest that it can achieve similar performance in spite of no expert involvement."	https://dx.doi.org/10.1109/ICAS.2008.35	Included	conflict_resolution		6
RL4SE	An2022	Traffic Signal Control Method Based on Modified Proximal Policy Optimization	For the traffic congestion problem at intersections, this paper proposed a modified proximal policy optimization algorithm, which can adaptively regulate traffic signals to alleviate intersection congestion. The algorithm made further theoretical corrections to the unbiasedness of the advantage function in it through the importance sampling method. The algorithm was also modified to better adapt to traffic signal control and thus increase the sensitivity of the algorithm to control actions. In order to fully extract road state information and reduce the complexity of the state space, the experiment used images as state inputs, which are real-time snapshots of road conditions at intersections. To verify the performance of the proposed algorithm, it was compared with other deep reinforcement learning algorithms in the traffic simulation software SUMO. The results show that the cumulative reward of the model constructed by the proposed algorithm is increased by 68.3\% and the average waiting time is reduced by 65.3\% compared to the baseline DQN model. Experimental results demonstrate that the algorithm can effectively regulate traffic congestion.	https://dx.doi.org/10.1109/ICTLE55577.2022.9901894	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Andersson1999	Reactive and Memory-Based Genetic Programming for Robot Control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Andre2002	State abstraction for programmable reinforcement learning agents	Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current,state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining hierarchical optimality, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is. essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich's taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Andrews2020	Tracking the State of Large Dynamic Networks via Reinforcement Learning	A Network Inventory Manager (NIM) is a software solution that scans, processes and records data about all devices in a network. We consider the problem faced by a NIM that can send out a limited number of probes to track changes in a large, dynamic network. The underlying change rate for the Network Elements (NEs) is unknown and may be highly non-uniform. The NIM should concentrate its probe budget on the NEs that change most frequently with the ultimate goal of minimizing the weighted Fraction of Stale Time (wFOST) of the inventory. However, the NIM cannot discover the change rate of a NE unless the NE is repeatedly probed.We develop and analyze two algorithms based on Reinforcement Learning to solve this exploration-vs-exploitation problem. The first is motivated by the Thompson Sampling method and the second is derived from the Robbins-Monro stochastic learning paradigm. We show that for a fixed probe budget, both of these algorithms produce a potentially unbounded improvement in terms of wFOST compared to the baseline algorithm that divides the probe budget equally between all NEs. Our simulations of practical scenarios show optimal performance in minimizing wFOST while discovering the change rate of the NEs.	https://dx.doi.org/10.1109/INFOCOM41043.2020.9155484	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Angles2021	Improving the Teaching-Learning Process in Engineering Through a Game-Based Web Support System: Edutrivias	"In this article, the authors propose the use of trivia games as a tool to support the teaching-learning process in engineering, and present results of the use of trivia games in the Chilean university context. Specifically, we describe a web system called Edutrivias (https://edutrivias.cl) which allows the interaction between teacher and engineering students through game-based learning. Edutrivias allows teachers to create trivia games that can be played by students, as many times as they want, within determined goals and a period of time defined by the teacher. Every time a student plays a trivia, he/she receives information regarding his/her level of progress. At the same time, the teacher can verify the participation of the students, as well as the level of the group and individual advancement. From a pedagogical point of view, teachers deliver knowledge through the trivia games, while the students ""play"" trivia games to acquire, increase and apply their knowledge through an intermittent reinforcement learning program. The main goals of this article are: (1) to present the foundations of the use of game-based learning in the competence development of engineering students, (2) to describe the main components and functionalities of Edutrivias, and (3) to present the results of an exploratory case study of Edutrivias with information of Chilean students of an engineering school at the University of Talca."	https://dx.doi.org/10.1007/978-3-030-68198-2_45	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aoki2020	Cooperative Perception with Deep Reinforcement Learning for Connected Vehicles	Sensor-based perception on vehicles are becoming prevalent and important to enhance road safety. Autonomous driving systems use cameras, LiDAR and radar to detect surrounding objects, while human-driven vehicles use them to assist the driver. However, the environmental perception by individual vehicles has the limitations on coverage and/or detection accuracy. For example, a vehicle cannot detect objects occluded by other moving/static obstacles. In this paper, we present a cooperative perception scheme with deep reinforcement learning to enhance the detection accuracy for the surrounding objects. By using deep reinforcement learning to select the data to transmit, our scheme mitigates the network load in vehicular networks and enhances the communication reliability. To design, test and verify the practical and resource-efficient cooperative perception framework, we develop a Cooperative & Intelligent Vehicle Simulation (CIVS) Platform where we integrate three software components: a traffic simulator, a vehicle simulator, and an object classifier. The simulation platform constitutes a unified framework to evaluate a traffic model, vehicle model, communication model, and object classification model. Simulation results show that our scheme decreases packet loss and thereby increases the detection accuracy by up to 12\%, compared to the baseline protocol.	https://dx.doi.org/10.1109/IV47402.2020.9304570	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Araiza-Illan2016	Intelligent Agent-Based Stimulation for Testing Robotic Software in Human-Robot Interactions		https://doi.org/10.1145/3022099.3022101	Included	conflict_resolution		6
RL4SE	Araujo2020	URNAI: A Multi-Game Toolkit for Experimenting Deep Reinforcement Learning Algorithms	In the last decade, several game environments have been popularized as testbeds for experimenting reinforcement learning algorithms, an area of research that has shown great potential for artificial intelligence based solutions. These game environments range from the simplest ones like CartPole to the most complex ones such as StarCraft II. However, in order to experiment an algorithm in each of these environments, researchers need to prepare all the settings for each one, a task that is very time consuming since it entails integrating the game environment to their software and treating the game environment variables. So, this paper introduces URNAI, a new multi-game toolkit that enables researchers to easily experiment with deep reinforcement learning algorithms in several game environments. To do this, URNAI implements layers that integrate existing reinforcement learning libraries and existing game environments, simplifying the setup and management of several reinforcement learning components, such as algorithms, state spaces, action spaces, reward functions, and so on. Moreover, URNAI provides a framework prepared for GPU supercomputing, which allows much faster experiment cycles. The first toolkit results are very promising.	https://dx.doi.org/10.1109/SBGames51465.2020.00032	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	6
RL4SE	Araujo1998	Connectionist reinforcement learning of robot control skills	Many robot manipulator tasks are difficult to model explicitly and it is difficult to design and program automatic control algorithms for them. The development, improvement, and application of learning techniques taking advantage of sensory information would enable the acquisition of new robot skills and avoid some of the difficulties of explicit programming. In this paper we use a reinforcement learning approach far on-line generation of skills for control of robot manipulator systems. Instead of generating skills by explicit programming of a perception to action mapping they are generated by trial and error learning, guided by a performance evaluation feedback function. The resulting system may be seen as an anticipatory system that constructs an internal representation model of itself and of its environment. This enables it to identify its current situation and to generate corresponding appropriate commands to the system in order to perform the required skill. The method was applied to the problem of learning a force control skill in which the tool-tip of a robot manipulator must be moved from a free space situation, to a contact state with a compliant surface and having a constant interaction force.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arend2022	MLPro - An integrative middleware framework for standardized machine learning tasks in Python	In recent years, many powerful software packages have been released on various aspects of machine learning (ML). However, there is still a lack of holistic development environments for the standardized creation of ML applications. The current practice is that researchers, developers, engineers and students have to piece together functionalities from several packages in their own applications. This prompted us to develop the integrative middleware framework MLPro that embeds flexible and recombinable ML models into standardized processes for training and real operations. In addition, it integrates numerous common open source frameworks and thus standardizes their use. A meticulously designed architecture combined with a powerful foundation of overarching basic functionalities ensures maximum recombinability and extensibility. In the first version of MLPro, we provide sub-frameworks for reinforcement learning (RL) and game theory (GT).	https://dx.doi.org/10.1016/j.simpa.2022.100421	Excluded	conflict_resolution	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arend2022a	MLPro emdash An integrative middleware framework for standardized machine learning tasks in Python[Formula presented]		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138322283&doi=10.1016\%2fj.simpa.2022.100421&partnerID=40&md5=d1e8b02f34263a10e0f48c7b0576fa79	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	6
RL4SE	Arif2022	A smart reactive jamming approach to counter reinforcement learning-based antijamming strategies in GEO SATCOM scenario	Reinforcement learning (RL) is being considered for future SATCOM systems due to its inherent capability to self-learn the optimum decision-making policy under different scenarios. This capability enables SATCOM systems to manage their resources judiciously and mitigate jamming attacks autonomously without prior jammer type classification. We propose a novel smart reactive SATCOM jamming approach that would not only counter these RL based anti-jamming strategies but would also be effective against conventional anti-jamming schemes, that is, FHSS and DSSS. The proposed jamming approach exploits the limitations in learning patterns of Q-learning-based RL agent and achieves effective jamming while conserving considerable amount of jamming power. To achieve this, we propose an intelligent jamming engine (IJE) along with few potent jamming algorithms and evaluate their performance in terms of throughput degradation of victim SATCOM link, jamming power conservation, and design complexity of the jammer. Software simulations successfully demonstrate the effectiveness of our proposed smart reactive jamming approach which outperforms the standard reactive jammer against RL-based antijamming schemes.	https://dx.doi.org/10.1002/sat.1418	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arinze2020	Novel Scheme For Congestion Control In Cellular Networks Using Deep Reinforcement Learning And Markov Decision Process Models	This research deals with the general issue of quality of service (QoS) provisioning and resource utilization in telecommunication networks. The issue requires that mobile network income be optimized while simultaneously satisfying QoS constraints that prevent getting into specific states and utilization of specific actions. However, supporting QoS requirements of different traffic types is more complicated due to the need to minimize two performance indicators - the probability of discarding a handover call and the probability of hindering a new call. Several approaches proposed recently try to provide efficient model-based solution to the problem by formulating it as an average reward neuro-dynamic programming (NDP) optimization problem together with decomposition function, but this is limited by Bellman's curse of dimensionality. In this paper, we proposed a novel hybrid optimization scheme to address the problem using Deep Reinforcement Learning (DRL), Markov Decision Process (MDP) and adaptive joint call admission control (AJCAC) respectively. In the proposed scheme, two classes of arrival traffic at the base station (BS) are considered; voice (real-time) and data (non-real-time) calls. Furthermore, traffic is classified as new and handoff according to the type of request. The scheme introduces an adaptive threshold value, which dynamically adjusts the network resources under high traffic intensity. In addition, the scheme introduces a learning agent whose state is described by an MDP. The MATLAB version 2010 software, OMNET++ simulator and SPSS will be used for data, numerical, algorithm simulation and modeling. Data analysis and simulation results will be carried out for performance evaluation of the proposed DQL-AJCAC scheme against existing models.	https://dx.doi.org/10.1109/ICMCECS47690.2020.247001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Armstrong2006	Dynamic Algorithm Selection Using Reinforcement Learning	It is often the case that many algorithms exist to solve a single problem, each possessing different performance characteristics. The usual approach in this situation is to manually select the algorithm which has the best average performance. However, this strategy has drawbacks in cases where the optimal algorithm changes during an invocation of the program, in response to changes in the program's state and the computational environment. This paper presents a prototype tool that uses reinforcement learning to guide algorithm selection at runtime, matching the algorithm used to the current state of the computation. The tool is applied to a simulation similar to those used in some computational chemistry problems. It is shown that the low dimensionality of the problem enables the optimal choice of algorithm to be determined quickly, and that the learning system can react rapidly to phase changes in the target program	https://dx.doi.org/10.1109/AIDM.2006.4	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Armstrong2008	Reinforcement learning for automated performance tuning: Initial evaluation for sparse matrix format selection	The field of reinforcement learning has developed techniques for choosing beneficial actions within a dynamic environment. Such techniques learn from experience and do not require teaching. This paper explores how reinforcement learning techniques might be used to determine efficient storage formats for sparse matrices. Three different storage formats are considered: coordinate, compressed sparse row, and blocked compressed sparse row. Which format performs best depends heavily on the nature of the matrix and the computer system being used. To test the above a program has been written to generate a series of sparse matrices, where any given matrix performs optimally using one of the three different storage types. For each matrix several sparse matrix vector products are performed. The goal of the learning agent is to predict the optimal sparse matrix storage format for that matrix. The proposed agent uses five attributes of the sparse matrix: the number of rows, the number of columns, the number of non-zero elements, the standard deviation of non-zeroes per row and the mean number of neighbours. The agent is characterized by two parameters: an exploration rate and a parameter that determines how the state space is partitioned. The ability of the agent to successfully predict the optimal storage format is analyzed for a series of 1,000 automatically generated test matrices.	https://dx.doi.org/10.1109/CLUSTR.2008.4663802	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arslan2022	Actor-critic reinforcement learning for bidding in bilateral negotiation		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139306597&doi=10.55730\%2f1300-0632.3899&partnerID=40&md5=b0cc57d23699d598e930ebe41693636b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Arza2022	Comparing Two Samples Through Stochastic Dominance: A Graphical Approach	Nondeterministic measurements are common in real-world scenarios: the performance of a stochastic optimization algorithm or the total reward of a reinforcement learning agent in a chaotic environment are just two examples in which unpredictable outcomes are common. These measures can be modeled as random variables and compared among each other via their expected values or more sophisticated tools such as null hypothesis statistical tests. In this article, we propose an alternative framework to visually compare two samples according to their estimated cumulative distribution functions. First, we introduce a dominance measure for two random variables that quantifies the proportion in which the cumulative distribution function of one of the random variables scholastically dominates the other one. Then, we present a graphical method that decomposes in quantiles (i) the proposed dominance measure and (ii) the probability that one of the random variables takes lower values than the other. With illustrative purposes, we reevaluate the experimentation of an already published work with the proposed methodology and we show that additional conclusions-missed by the rest of the methods-can be inferred. Additionally, the software package RVCompare was created as a convenient way of applying and experimenting with the proposed framework.	https://dx.doi.org/10.1080/10618600.2022.2084405	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	6
RL4SE	Arzymatov2020	SANgo: a storage infrastructure simulator with reinforcement learning support	We introduce SANgo (Storage Area Network in the Go language)-a Go-based package for simulating the behavior of modern storage infrastructure. The software is based on the discrete-event modeling paradigm and captures the structure and dynamics of high-level storage system building blocks. The flexible structure of the package allows us to create a model of a real storage system with a configurable number of components. The granularity of the simulated system can be defined depending on the replicated patterns of actual system behavior. Accurate replication enables us to reach the primary goal of our simulator-to explore the stability boundaries of real storage systems. To meet this goal, SANgo offers a variety of interfaces for easy monitoring and tuning of the simulated model. These interfaces allow us to track the number of metrics of such components as storage controllers, network connections, and harddrives. Other interfaces allow altering the parameter values of the simulated system effectively in real-time, thus providing the possibility for training a realistic digital twin using, for example, the reinforcement learning (RL) approach. One can train an RL model to reduce discrepancies between simulated and real SAN data. The external control algorithm can adjust the simulator parameters to make the difference as small as possible. SANgo supports the standard OpenAI gym interface; thus, the software can serve as a benchmark for comparison of different learning algorithms.	https://www.ncbi.nlm.nih.gov/pubmed/33816922	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Asadpour2004	Compact Q-learning optimized for micro-robots with processing and memory constraints	Scaling down robots to miniature size introduces many new challenges including memory and program size limitations, low processor performance and low power autonomy. In this paper we describe the concept and implementation of learning of a safe-wandering task with the autonomous micro-robots, Alice. We propose a simplified reinforcement learning algorithm based on one-step Q-learning that is optimized in speed and memory consumption. This algorithm uses only integer-based sum operators and avoids floating-point and multiplication operators. Finally, quality of learning is compared to a floating-point based algorithm. (C) 2004 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.robot.2004.05.006	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Asteroth1992	TRACKING AND GRASPING OF MOVING-OBJECTS - A BEHAVIOR-BASED APPROACH	Behaviour-based robotics (cf. Brooks [2]) has mainly been applied to the domain of autonomous systems and mobile robots. In this paper we show how this approach to robot programming can be used to design a flexible and robust controller for a five degrees of freedom (DOF) robot arm. The implementation of the robot controller to be presented features the sensor and motor patterns necessary to tackle a problem we consider to be hard to solve for traditional controllers. These sensor and motor patterns are linked together forming various behaviours. The global control structure based on Brooks' subsumption architecture will be outlined. It coordinates the individual behaviours into goal-directed behaviour of the robot without the necessity to program this emerging global behaviour explicitly and in advance. To conclude, some shortcomings of the current implementation are discussed and future work, especially in the field of reinforcement learning of individual behaviours, is sketched.		Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Asteroth1992a	Tracking and grasping of moving objects endash a behaviour-based approach endash		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029539075&doi=10.1007\%2fbfb0024971&partnerID=40&md5=386c16b30eb7eb419da2415c58ce8653	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E2: Software engineering is not the problem RL is used for	6
RL4SE	Auer2011	Invited talk: UCRL and autonomous exploration		https://doi.org/10.1007/978-3-642-29946-9_1	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper	6
RL4SE	Aumjaud2021	rl_reach: Reproducible reinforcement learning experiments for robotic tasks	Training reinforcement learning agents at solving a given task is highly dependent on identifying optimal sets of hyperparameters and selecting suitable environment input/output configurations. This tedious process could be eased with a straightforward toolbox allowing its user to quickly compare different training parameter sets. We present rl_reach, a self-contained, open-source and easy-to-use software package designed to run reproducible reinforcement learning experiments for customisable robotic reaching tasks. rl_reach packs together training environments, agents, hyperparameter optimisation tools and policy evaluation scripts, allowing its users to quickly investigate and identify optimal training configurations. rl_reach is publicly available at this URL: https://github.com/PierreExeter/rl_reach.	https://dx.doi.org/10.1016/j.simpa.2021.100061	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aumjaud2021a	rl_reach: Reproducible reinforcement learning experiments for robotic reaching tasks[Formula presented]		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115865247&doi=10.1016\%2fj.simpa.2021.100061&partnerID=40&md5=32582555c93ccc79f4052767a1241dc0	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aung2010	Reinforcement learning using voronoi space division			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Aunger2016	Behaviour Centred Design: towards an applied science of behaviour change	Behaviour change has become a hot topic. We describe a new approach, Behaviour Centred Design (BCD), which encompasses a theory of change, a suite of behavioural determinants and a programme design process. The theory of change is generic, assuming that successful interventions must create a cascade of effects via environments, through brains, to behaviour and hence to the desired impact, such as improved health. Changes in behaviour are viewed as the consequence of a reinforcement learning process involving the targeting of evolved motives and changes to behaviour settings, and are produced by three types of behavioural control mechanism (automatic, motivated and executive). The implications are that interventions must create surprise, revalue behaviour and disrupt performance in target behaviour settings. We then describe a sequence of five steps required to design an intervention to change specific behaviours: Assess, Build, Create, Deliver and Evaluate. The BCD approach has been shown to change hygiene, nutrition and exercise-related behaviours and has the advantages of being applicable to product, service or institutional design, as well as being able to incorporate future developments in behaviour science. We therefore argue that BCD can become the foundation for an applied science of behaviour change.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982279417&doi=10.1080\%2f17437199.2016.1219673&partnerID=40&md5=1675458abbfb3683d537a51f5f46c0d7	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Ayimba2021	SQLR: Short-Term Memory Q-Learning for Elastic Provisioning	As a growing number of service and application providers choose cloud networks to deliver their services on a software-as-a-service (SaaS) basis, cloud providers need to make their provisioning systems agile enough to meet service level agreements (SLAs). At the same time, they should guard against over-provisioning, which limits their capacity to accommodate more tenants. To this end, we propose Shortterm memory Q-Learning pRovisioning (SQLR, pronounced as ``scaler''), a system employing a customized variant of the modelfree reinforcement learning algorithm. It can reuse contextual knowledge learned from one workload to optimize the number of virtual machines (resources) allocated to serve other workload patterns. With minimal overhead, SQLR achieves comparable results to systems where resources are unconstrained. Our experiments show that we can reduce the amount of provisioned resources by about 20\% with less than 1\% overall service unavailability (due to blocking), while delivering similar response times to those of an over-provisioned system.	https://dx.doi.org/10.1109/TNSM.2021.3075619	Included	conflict_resolution		6
RL4SE	Baccour2020	RL-OPRA: Reinforcement Learning for Online and Proactive Resource Allocation of crowdsourced live videos	With the advancement of rich media generating devices, the proliferation of live Content Providers (CP), and the availability of convenient internet access, crowdsourced live streaming services have witnessed unexpected growth. To ensure a better Quality of Experience (QoE), higher availability, and lower costs, large live streaming CPs are migrating their services to geo-distributed cloud infrastructure. However, because of the dynamics of live broadcasting and the wide geo-distribution of viewers and broadcasters, it is still challenging to satisfy all requests with reasonable resources. To overcome this challenge, we introduce in this paper a prediction driven approach that estimates the potential number of viewers near different cloud sites at the instant of broadcasting. This online and instant prediction of distributed popularity distinguishes our work from previous efforts that provision constant resources or alter their allocation as the popularity of the content changes. Based on the derived predictions, we formulate an Integer-Linear Program (ILP) to proactively and dynamically choose the right data center to allocate exact resources and serve potential viewers, while minimizing the perceived delays. As the optimization is not adequate for online serving, we propose a real-time approach based on Reinforcement Learning (RL), namely RL-OPRA, which adaptively learns to optimize the allocation and serving decisions by interacting with the network environment. Extensive simulation and comparison with the ILP have shown that our RL-based approach is able to present optimal results compared to heuristic-based approaches. (c) 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	https://dx.doi.org/10.1016/j.future.2020.06.038	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Backeus2005	Impact of climate change uncertainty on optimal forest management policies at stand level			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Badica2017	Integration of Jason Reinforcement Learning Agents into an Interactive Application	The context of this paper is research on the application of agent-oriented programming to experiment with reinforcement learning algorithms. Traditionally, reinforcement learning fits well within the theory of agent systems. Nevertheless, most experimental approaches employ standard software engineering tools, rather than specific agent-oriented technologies developed within the agent community. The goal of our work is to stimulate synergies and development of agent-oriented programming technologies to better fit the context of agent systems research. In particular in this paper we focus on the development of an experimental interactive application that incorporates reinforcement learning agents created using the Jason agent-oriented programming language.	https://dx.doi.org/10.1109/SYNASC.2017.00065	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baek2019	Managing Fog Networks using Reinforcement Learning Based Load Balancing Algorithm	The powerful paradigm of Fog computing is currently receiving major interest, as it provides the possibility to integrate virtualized servers into networks and brings cloud service closer to end devices. To support this distributed intelligent platform, Software-Defined Network (SDN) has emerged as a viable network technology in the Fog computing environment. However, uncertainties related to task demands and the different computing capacities of Fog nodes, inquire an effective load balancing algorithm. In this paper, the load balancing problem has been addressed under the constraint of achieving the minimum latency in Fog networks. To handle this problem, a reinforcement learning based decision-making process has been proposed to find the optimal offloading decision with unknown reward and transition functions. The proposed process allows Fog nodes to offload an optimal number of tasks among incoming tasks by selecting an available neighboring Fog node under their respective resource capabilities with the aim to minimize the processing time and the overall overloading probability. Compared with the traditional approaches, the proposed scheme not only simplifies the algorithmic framework without imposing any specific assumption on the network model but also guarantees convergence in polynomial time. The results show that, during average delays, the proposed reinforcement learning-based offloading method achieves significant performance improvements over the variation of service rate and traffic arrival rate. The proposed algorithm achieves 1.17\%, 1.02\%, and 3.21\% lower overload probability relative to random, least-queue and nearest offloading selection schemes, respectively.	https://dx.doi.org/10.1109/WCNC.2019.8885745	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baez2013	Application of an educational strategy based on a soccer robotic platform	In this paper we describe the design and implementation of an educational methodology based on a robotic platform used for the small size league (SSL) challenge of the RoboCup initiative. The methodology is based on three main aspects of the learning process, namely classical conditioning, reinforcement learning and cognitive learning. This is achieved through the combination of robotic concepts applied to the soccer problem; a highly interesting topic to the students, together with skill oriented modules. We show practical results achieved after applying this methodology in specific courses in an undergraduate electrical engineering program. Our initial results demonstrate that it is possible to attain significantly better results in terms of learnt concepts and motivation when using our robotic soccer based strategy.	https://dx.doi.org/10.1109/ICAR.2013.6766533	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bag2019	A combined reinforcement learning and sliding mode control scheme for grid integration of a PV system	The paper presents development of a reinforcement learning (RL) and sliding mode control (SMC) algorithm for a 3-phase PV system integrated to a grid. The PV system is integrated to grid through a voltage source inverter (VSI), in which PV-VSI combination supplies active power and compensates reactive power of the local non-linear load connected to the point of common coupling (PCC). For extraction of maximum power from the PV panel, we develop a RL based maximum power point tracking (MPPT) algorithm. The instantaneous power theory (IPT) is adopted for generation reference inverter current (RIC). An SMC algorithm has been developed for injecting current to the local non-linear load at a reference value. The RL-SMC scheme is implemented in both simulation using MATLAB/SIMULINK software and on a prototype PV experimental. The performance of the proposed RL-SMC scheme is compared with that of fuzzy logic-sliding mode control (FL-SMC) and incremental conductance-sliding mode control (IC-SMC) algorithms. From the obtained results, it is observed that the proposed RL-SMC scheme provides better maximum power extraction and active power control than the FL-SMC and IC-SMC schemes.	https://dx.doi.org/10.17775/CSEEJPES.2017.01000	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bahamid2022	A review on crowd analysis of evacuation and abnormality detection based on machine learning systems	Human crowds have become hotspot research, particularly in crowd analysis to ensure human safety. Adaptations of machine learning (ML) approaches, especially deep learning, play a vital role in the applications of evacuation, detection, and prediction pertaining to crowd analysis. Further development in the analysis of crowd is needed to understand human behaviors due to the fast growth of crowd in urban megacities. This article presents a comprehensive review of crowd analysis ML-based systems, where it is categorized with respect to its purposes, viz. crowd evacuation that provides efficient evacuation routes, abnormality detection that could detect the occurrence of any irregular movement or behavior, and crowd prediction that could foresee the occurrence of any possible disasters or predict pedestrian trajectory. Moreover, this article reviews the applied techniques of machine learning with a brief discussion on the used software and simulation platforms. This work also classifies crowd evacuation into data-driven methods and goal-driven learning methods that have attracted significant attention due to their potential to adopt virtual agents with learning capabilities. This review finds that convolutional neural networks and recurrent neural networks have shown superiority in abnormality detection and prediction, whereas deep reinforcement learning has shown potential performance in the development of human level capacities of reasoning. These three methods contribute to the modeling and understanding of pedestrian behavior and will enhance further development in crowd analysis to ensure human safety.	https://dx.doi.org/10.1007/s00521-022-07758-5	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bahr2019	Development and Validation of Active Roll Control based on Actor-critic Neural Network Reinforcement Learning		https://doi.org/10.5220/0007787400360046	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bahrami2021	Deep Reinforcement Learning for Demand Response in Distribution Networks		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096865050&doi=10.1109\%2fTSG.2020.3037066&partnerID=40&md5=c248d562004386f75d41fa72995c8f3d	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bai2022	Lamarckian Platform: Pushing the Boundaries of Evolutionary Reinforcement Learning Towards Asynchronous Commercial Games	Despite the emerging progress of integrating evolutionary computation into reinforcement learning, the absence of a high-performance platform endowing composability and massive parallelism causes non-trivial difficulties for research and applications related to asynchronous commercial games. Here we introduce Lamarckian11The code and demonstrational setup of Lamarckian are publicly available at https://github. com/lamarckian/lamarckian. \endash an open-source platform featuring support for evolutionary reinforcement learning scalable to distributed computing resources. To improve the training speed and data efficiency, Lamarckian adopts optimized communication methods and an asynchronous evolutionary reinforcement learning workflow. To meet the demand for an asynchronous interface by commercial games and various methods, Lamarckian tailors an asynchronous Markov Decision Process interface and designs an object-oriented software architecture with decoupled modules. In comparison with the state-of-the-art RLlib, we empirically demonstrate the unique advantages of Lamarckian on benchmark tests with up to 6000 CPU cores: i) both the sampling efficiency and training speed are doubled when running PPO on Google football game; ii) the training speed is 13 times faster when running PBT+PPO on Pong game. Moreover, we also present two use cases: i) how Lamarckian is applied to generating behavior-diverse game AI; ii) how Lamarckian is applied to game balancing tests for an asynchronous commercial game.	https://dx.doi.org/10.1109/TG.2022.3208324	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bai2006	A Mobile Agents-Based Real-time Mechanism for Wireless Sensor Network Access on the Internet	Due to the variety of applications and their importance, wireless sensor networks (WSNs) would need to be connected to the Internet. Some approaches have been proposed to connect wireless sensor networks to the existing TCP/IP networks, such as the application-level gateways or overlay networks. However, most existing approaches have to consume network bandwidth and node energy to maintain the static network structure which is neither scalable nor reliable. In this paper, we describe the mobile agent based real-time (MBR) mechanism which use of the mobile software agent (MSA) paradigm to design a dynamic infrastructure for WSNs access on the Internet. We present a agent migration protocol based on reinforcement learning method to reduce the query delay and improve the total performance	https://dx.doi.org/10.1109/ICIA.2006.306016	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baker2011	Individual differences in substance dependence: At the intersection of brain, behaviour and cognition	Recent theories of drug dependence propose that the transition from occasional recreational substance use to harmful use and dependence results from the impact of disrupted midbrain dopamine signals for reinforcement learning on frontal brain areas that implement cognitive control and decision-making. We investigated this hypothesis in humans using electrophysiological and behavioral measures believed to assay the integrity of midbrain dopamine system and its neural targets. Our investigation revealed two groups of dependent individuals, one characterized by disrupted dopamine-dependent reward learning and the other by disrupted error learning associated with depression-proneness. These results highlight important neurobiological and behavioral differences between two classes of dependent users that can inform the development of individually tailored treatment programs.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959823204&doi=10.1111\%2fj.1369-1600.2010.00243.x&partnerID=40&md5=e52c842a99576e3ddaf9e327c0554916	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bakshi2018	Considerations for artificial intelligence and machine learning: Approaches and use cases	As data sets grow, leveraging machines to learn valuable patterns from structured data can be extremely powerful. The volume of data is too large for comprehensive analysis, and the range of potential correlations and relationships between disparate data sources are too great for any analyst to test all hypotheses and derive all the value buried in the data. Machine learning (ML) is ideal for exploiting the opportunities hidden in big data. Machine learning is a type of artificial intelligence (AI) that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. The basics of machine learning is to build algorithms that can take input data and use statistical analysis to predict an output value within an acceptable range. This paper explores the basics of machine learning, discussing concepts and topics like supervised, unsupervised and reinforcement learning, regression, classification, model evaluation metrics, overfitting, variance versus bias, linear regression, ensemble methods, model selection, Decision Trees, Random Forests. The paper then will review several several use cases, where machine learning can be applied, including but not limited to Aerospace, Internet of Things (IoT) and Computer Network Analytics use cases. The applicability of AI and ML will be reviewed in these use cases. Finally, the latest trends in machine learning will be discussed.	https://dx.doi.org/10.1109/AERO.2018.8396488	Excluded	new_screen	E3: Only conceptual results are reported,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balaji2010	Urban traffic signal control using reinforcement learning agents	This study presents a distributed multi-agent-based traffic signal control for optimising green timing in an urban arterial road network to reduce the total travel time and delay experienced by vehicles. The proposed multi-agent architecture uses traffic data collected by sensors at each intersection, stored historical traffic patterns and data communicated from agents in adjacent intersections to compute green time for a phase. The parameters like weights, threshold values used in computing the green time is fine tuned by online reinforcement learning with an objective to reduce overall delay. PARAMICS software was used as a platform to simulate 29 signalised intersection at Central Business District of Singapore and test the performance of proposed multi-agent traffic signal control for different traffic scenarios. The proposed multi-agent reinforcement learning (RLA) signal control showed significant improvement in mean time delay and speed in comparison to other traffic control system like hierarchical multi-agent system (HMS), cooperative ensemble (CE) and actuated control.	https://dx.doi.org/10.1049/iet-its.2009.0096	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balakiruthiga2021	(ITMP) endash Intelligent Traffic Management Prototype using Reinforcement Learning approach for Software Defined Data Center (SDDC)		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116381356&doi=10.1016\%2fj.suscom.2021.100610&partnerID=40&md5=5f4775ffcdd88f000db68647c34bf778	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balakiruthiga2021a	(ITMP)-Intelligent Traffic Management Prototype using Reinforcement Learning approach for Software Defined Data Center (SDDC)	Software defined network architecture offers scalability and resilience as the significant advantages to data center networks. This increases the fault tolerance ability of traditional data center network architectures. Massive amounts of mobile network data as well as e-commerce application data requests are the key sources for data centers which recurrently desire attention. Researchers are yet to design a suitable prototype with functional intelligence to support traffic optimization techniques in SDDC. In this research work, we are proposing an intelligent traffic management prototype for software defined data center by means of reinforcement learning approach through the integration of the functionalities such as controller positioning, traffic load balancing, routing and energy efficiency. These are the key areas where traffic optimization becomes essential to improve network performance. The proposed prototype provides a complete framework for enterprises to deploy applications in an efficient manner. We model the prototype to handle dynamic network data applications such as information retrieval, communication and banking applications. We focus in this article on how communication happens among the data center nodes as an inter-data center communication process upon receiving requests from the applications considered. To further enhance the novelty and efficiency of our research work, we adopt multiple reinforcement learning agents to lever load balancing and routing functionalities. Moreover, to assess and ensure the optimized network performance, we evaluate the energy consumption of the network achieved through our proposed prototype.	https://dx.doi.org/10.1016/j.suscom.2021.100610	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Balakrishnan2021	Transfer Reinforcement Learning for Autonomous Driving: From WiseMove to WiseSim		https://doi.org/10.1145/3449356	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Banarse2019	The Body is Not a Given: Joint Agent Policy Learning and Morphology Evolution			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Baranwal2021	ReLAccS: A Multilevel Approach to Accelerator Design for Reinforcement Learning on FPGA-Based Systems	Reinforcement learning (RL), specifically Q-learning, with human-like learning abilities to learn from experience without any a priori data, is being increasingly used in embedded systems in the field of control and navigation. However, finding the optimal policy in this approach can be highly compute-intensive, and a software-only implementation may not satisfy the application's timing constraints. To this end, we propose optimization methods at multiple levels of accelerator design for RL. Specifically, at the architecture-level, we exploit the instruction-level parallelism and the spatial parallelism in FPGAs to improve the throughput over state-of-the-art designs by up to 34\%. Further, we propose lookup table-level optimizations to reduce the resource utilization and power dissipation of the accelerator. Finally, we propose algorithm-level approximation that can be used for acceleration of Q-learning problems with more states and for reducing the peak power dissipation. We report up to 10$\times$ reduction in power dissipation with marginal degradation in quality of results.	https://dx.doi.org/10.1109/TCAD.2020.3028350	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barash2019	Reports of the Workshops Held at the 2019 AAAI Conference on Artificial Intelligence	The workshop program of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19) was held in Honolulu, Hawaii, on Sunday and Monday, January 27 and 28, 2019. There were 16 workshops in the program: Affective Content Analysis: Modeling Affect-in-Action; Agile Robotics for Industrial Automation Competition; Artificial Intelligence for Cyber Security; Artificial Intelligence Safety; Dialog System Technology Challenge; Engineering Dependable and Secure Machine Learning Systems; Games and Simulations for Artificial Intelligence; Health Intelligence; Knowledge Extraction from Games; Network Interpretability for Deep Learning; Plan, Activity, and Intent Recognition; Reasoning and Learning for Human-Machine Dialogues; Reasoning for Complex Question Answering; Recommender Systems Meet Natural Language Processing; Reinforcement Learning in Games; and Reproducible AI. This report contains brief summaries of all the workshops that were held.	https://dx.doi.org/10.1609/aimag.v40i3.4981	Excluded	new_screen	E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper,E5: Other not a paper	6
RL4SE	Barbieri2018	DEVS Modeling and Simulation of Financial Leverage Effect Based on Markov Decision Process	Decision making during a financial asset optimization process leading to a potential leverage effect is a major issue in the management of an investment program such as European development programs. Modeling and simulation based on reinforcement learning can propose a decision-making policy in this kind of process. This paper presents a DEVS discrete-event modeling and simulation approach from Markov decision-making processes applied to the search for maximum leverage on self-financing capabilities in grant application instruction phase. The application of the approach presented in this paper is made on the search for the leverage effect linked to the price volatility of the main stock market indices (CAC40, NasDaq, etc.).	https://dx.doi.org/10.1109/UV.2018.8642121	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barbucha2012	Search modes for the cooperative multi-agent system solving the vehicle routing problem		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860242268&doi=10.1016\%2fj.neucom.2011.07.032&partnerID=40&md5=2aae3d810ed1539355ec489c563e1b44	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bargiacchi2020	AI-Toolbox: A C plus plus library for Reinforcement Learning and Planning (with Python Bindings)	This paper describes AI-Toolbox, a C++ software library that contains reinforcement learning and planning algorithms, and supports both single and multi agent problems, as well as partial observability. It is designed for simplicity and clarity, and contains extensive documentation of its API and code. It supports Python to enable users not comfortable with C++ to take advantage of the library's speed and functionality.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Bargiacchi2022	AI-Toolbox: a C++ library for reinforcement learning and planning (with python bindings)			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barrett2010	A comparison of learning approaches to support the adaptive provision of distributed services			Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for,E5: Other not a paper,E5: Other not a paper,E2: Software engineering is not the problem RL is used for	6
RL4SE	Barriga2021	Addressing the trade off between smells and quality when refactoring class diagrams	Models are core artifacts of modern software engineering processes, and they are subject to evolution throughout their life cycle due to maintenance and to comply with new requirements as any other software artifact. Smells in modeling are indicators that something may be wrong within the model design. Removing the smells using refactoring usually has a positive effect on the general quality of the model. However, it could have a negative impact in some cases since it could destroy the quality wanted by stakeholders. PARMOREL is a framework that, using reinforcement learning, can automatically refactor models to comply with user preferences. The work presented in this paper extends PARMOREL to support smells detection and selective refactoring based on quality characteristics to assure only the refactoring with a positive impact is applied. We evaluated the approach on a large available public dataset to show that PARMOREL can decide which smells should be refactored to maintain and, even improve, the quality characteristics selected by the user.	https://dx.doi.org/10.5381/jot.2021.20.3.a1	Included	new_screen		6
RL4SE	Barriga2022	PARMOREL: a framework for customizable model repair		https://doi.org/10.1007/s10270-022-01005-0	Included	new_screen		6
RL4SE	Barriga2020	A comparative study of reinforcement learning techniques to repair models		https://doi.org/10.1145/3417990.3421395	Included	new_screen		6
RL4SE	Barriga2018	Automatic model repair using reinforcement learning			Included	new_screen		4
RL4SE	Barriga2019	Personalized and Automatic Model Repairing using Reinforcement Learning	When performing modeling activities, the chances of breaking a model increase together with the size of development teams and number of changes in software specifications. Model repair research mostly proposes two different solutions to this issue: fully automatic, non-interactive model repairing tools or support systems where the repairing choice is left to the developer's criteria. In this paper, we propose the use of reinforcement learning algorithms to achieve the repair of broken models allowing both automation and personalization. We validate our proposal by repairing a large set of broken models randomly generated with a mutation tool.	https://dx.doi.org/10.1109/MODELS-C.2019.00030	Included	new_screen		4
RL4SE	Barriga2019a	Towards quality assurance in repaired models with PARMOREL			Included	new_screen		4
RL4SE	Barriga2020_1	Improving model repair through experience sharing	In model-driven software engineering, models are used in all phases of the development process. These models may get broken due to various editions throughout their life-cycle. There are already approaches that provide an automatic repair of models, however, the same issues might not have the same solutions in all contexts due to different user preferences and business policies. Personalization would enhance the usability of automatic repairs in different contexts, and by reusing the experience from previous repairs we would avoid duplicated calculations when facing similar issues. By using reinforcement learning we have achieved the repair of broken models allowing both automation and personalization of results. In this paper, we propose transfer learning to reuse the experience learned from each model repair. We have validated our approach by repairing models using different sets of personalization preferences and studying how the repair time improved when reusing the experience from each repair.	https://dx.doi.org/10.5381/jot.2020.19.2.a13	Included	new_screen		4
RL4SE	Bartin2019	Use of learning classifier systems in microscopic toll plaza simulation models	This study presents the application of XCS learning algorithm in simulating drivers' lane selection behaviour in microscopic simulation models of toll plazas. XCS is a special case of learning classifier systems, a machine learning technique that ties reinforcement learning (RL) and genetic algorithm. The proposed formulation translates an agent's lane selection decision into a learning problem, assuming that its ultimate objective is to reduce delay and crash risk. The agent is simulated without any notion of the outcome of its decisions. Through multiple learning episodes and the outcome of its actions, it learns the best policy to implement in a given network setting. A hypothetical toll plaza simulation network developed in Paramics simulation software is used to conduct experimental analyses. The results demonstrate that the agent's toll lane selection decision yields superior results in terms of delay and crash risk compared with those of minimum queue lane selection, minimum risk lane selection, random lane selection and multinomial probit model-based lane selection behaviours. Although both the delay and crash risk objectives are not used simultaneously during learning, this study is intended as a proof of concept to demonstrate the feasibility of implementing RL algorithms in microscopic traffic simulation models.	https://dx.doi.org/10.1049/iet-its.2018.5121	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Barzilay2012	Learning to behave by reading			Excluded	new_screen	E5: Other not a paper,E5: Other not a paper	4
RL4SE	Batista2019	Utterance Copy in Formant-Based Speech Synthesizers Using LSTM Neural Networks	Utterance copy, also known as speech imitation, is the task of estimating the parameters of an input, target speech signal in order to artificially reconstruct another signal with the same properties at the output. This can be considered a difficult inverse problem, since the input-output relationship is often non-linear, apart from having several parameters to be estimated and adjusted. This work describes the development of an application that uses a long short-term memory neural network (LSTM) to learn how to estimate the input parameters of thel formant-based Klatt speech synthesizer. Formant-based synthesizers do not reach state-of-art performance for text-to-speech (TTS) applications, but are an important tool for linguists studies due to the high interpretability of its input parameters. The proposed system was compared to the WinSnoori baseline software on both artificially-produced target utterances, generated by the DECtalk TTS system; and natural target ones. Results show that our system outperforms the baseline for synthetic voices on the metrics of PESQ, SNR, RMSE and LSD. For natural voices, the experiments indicate the need for an architecture that does not depend on labeled data, such as reinforcement learning.	https://dx.doi.org/10.1109/BRACIS.2019.00025	Excluded	new_screen	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Bauer2011	Adaptation-based programming in Haskell		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954472193&doi=10.4204\%2fEPTCS.66.1&partnerID=40&md5=4fd3ea18bfd14ecb74c5eae8a33a23f6	Excluded	new_screen	E3: Only conceptual results are reported,E2: Software engineering is not the problem RL is used for	4
RL4SE	Baum2002	Toward code evolution by artificial economies	We have begun exploring code evolution by artificial economies. We implemented a reinforcement learning machine called Hayek2 consisting of agents, written in a machine language inspired by Ray's Tierra, that interact economically. The economic structure of Hayek2 addresses credit assignment at both the agent and meta levels. Hayek2 succeeds in evolving code to solve Blocks World problems, and has been more effective at this than our hillclimbing program and our genetic program (GP). Our hillclimber and our GP also performed well, learning algorithms as strong as a simple search program that incorporates hand-coded domain knowledge. We made efforts to optimize our hillclimbing program and it has features that may be of independent interest. Our GP using crossover performed far better than a version utilizing other macro-mutations or our hillclimber, bearing on a controversy in the genetic programming literature.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Baumgart2021	A Reinforcement Learning Approach for Traffic Control	Intelligent traffic control is a key tool to achieve and to realize resource-efficient and sustainable mobility solutions. In this contribution, we study a promising data-based control approach, reinforcement learning (RL), and its applicability to traffic flow problems in a virtual environment. We model different traffic networks using the microscopic traffic simulation software SUMO. RL-methods are used to teach controllers, so called RL agents, to guide certain vehicles or to control a traffic light system. The agents obtain real-time information from other vehicles and learn to improve the traffic flow by repetitive observation and algorithmic optimization. As controller models, we consider both simple linear models and non-linear radial basis function networks. The latter allow to include prior knowledge from the training data and a two-step training procedure leading to an efficient controller training.	https://dx.doi.org/10.5220/0010448501330141	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Beck2009	Reinforcement learning for Golog programs			Included	new_screen		4
RL4SE	Beck2012	Reinforcement learning for Golog programs with first-order state-abstraction	A special feature of programs in the action language Golog are non-deterministic constructs such as non-deterministic choice of actions or arguments. It has been shown that in the presence of stochastic actions and rewards reinforcement learning techniques can be applied to obtain optimal choices for those choice-points. In order to avoid an explosion of the state space, an abstraction mechanism is employed that computes first-order state descriptions for the given program. Intuitively, the idea is to generate abstract descriptions that group together states for which the expected reward of executing the program is the same. A current limitation is that a non-deterministic choice of arguments can be handled only if the possible candidates are known in advance. In this article we show how this restriction can be lifted. We also show how a first-order variant of binary decision diagrams can be used to efficiently compute first-order state abstractions. Moreover, we give a completely declarative specification of a learning Golog interpreter that incorporates the presented state-abstraction mechanisms.	https://dx.doi.org/10.1093/jigpal/jzs011	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Becker2010	CONTROLLING A SIMULATED ROBOT USING MACHINE LEARNING TECHNIQUES	USARSim group at NIST developed a simulated robot that operated in the Unreal Tournament 3 (UT3) gaming environment. They used a software PID controller to control the robot in UT3 worlds. Unfortunately, the PID controller did not work well, so NIST asked us to develop a better controller using machine learning techniques. In the process, we characterized the software PID controller and the robot's behavior in UT3 worlds. Using data collected from our simulations, we compared different machine learning techniques including linear regression and reinforcement learning (RL). Finally, we implemented a RL based controller in Mat lab and ran it in the UT3 environment via a TCP/IP link between Mat lab and UT3.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Begashaw2016	Enhancing Blind Interference Alignment with Reinforcement Learning	Blind interference alignment (IA) is a signaling scheme that suppresses interference in multi-user systems, without the knowledge of channel state information at the transmitter (CSIT). The key to performing IA without CSIT is the use of reconfigurable antennas (RA) that are capable of dynamically switching among a fixed number of radiation patterns to introduce artificial fluctuations in the channel. The radiation patterns used to realize blind IA have significant impacts on the overall performance of the system. Hence, an intelligent antenna pattern selection strategy is a crucial component of any practical RA-based blind IA implementation. In this work, we propose two reinforcement learning algorithms for selecting the optimal antenna configuration for blind IA. Furthermore, we evaluate the performance of these antenna mode selection techniques using over the air measurements on our software defined radio implementation of blind IA using a Reconfigurable Alford Loop Antenna that is capable of generating multiple radiation patterns. We quantify the performance of the algorithms in terms of received signal to interference and noise ratio (SINR) and show that our learning-based mode selection strategies are capable of choosing the highest performing mode 90\% of the time and attain over 2 dB gain in SINR over other selection approaches.	https://dx.doi.org/10.1109/GLOCOM.2016.7841815	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bekar2020	High fidelity progressive reinforcement learning for agile maneuvering uavs		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091908935&doi=10.2514\%2f6.2020-0898&partnerID=40&md5=d9fae619af21a737c069dead8fdb7fe9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bellemare2013	The arcade learning environment: an evaluation platform for general agents			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	BenBraiek2020	On testing machine learning programs	Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs. (C) 2020 Published by Elsevier Inc.	https://dx.doi.org/10.1016/j.jss.2020.110542	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Benatti2019	A Modular Simulation Platform for Training Robots via Deep Reinforcement Learning and Multibody Dynamics		https://doi.org/10.1145/3365265.3365274	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bera2021	Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning		https://doi.org/10.1145/3466752.3480114	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Berghout2022	Machine learning for cybersecurity in smart grids: A comprehensive review-based study on methods, solutions, and prospects		https://doi.org/10.1016/j.ijcip.2022.100547	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bermudez-Contreras2020	The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence	Recent advances in artificial intelligence (AI) and neuroscience are impressive. In AI, this includes the development of computer programs that can beat a grandmaster at GO or outperform human radiologists at cancer detection. A great deal of these technological developments are directly related to progress in artificial neural networks-initially inspired by our knowledge about how the brain carries out computation. In parallel, neuroscience has also experienced significant advances in understanding the brain. For example, in the field of spatial navigation, knowledge about the mechanisms and brain regions involved in neural computations of cognitive maps-an internal representation of space-recently received the Nobel Prize in medicine. Much of the recent progress in neuroscience has partly been due to the development of technology used to record from very large populations of neurons in multiple regions of the brain with exquisite temporal and spatial resolution in behaving animals. With the advent of the vast quantities of data that these techniques allow us to collect there has been an increased interest in the intersection between AI and neuroscience, many of these intersections involve using AI as a novel tool to explore and analyze these large data sets. However, given the common initial motivation point-to understand the brain-these disciplines could be more strongly linked. Currently much of this potential synergy is not being realized. We propose that spatial navigation is an excellent area in which these two disciplines can converge to help advance what we know about the brain. In this review, we first summarize progress in the neuroscience of spatial navigation and reinforcement learning. We then turn our attention to discuss how spatial navigation has been modeled using descriptive, mechanistic, and normative approaches and the use of AI in such models. Next, we discuss how AI can advance neuroscience, how neuroscience can advance AI, and the limitations of these approaches. We finally conclude by highlighting promising lines of research in which spatial navigation can be the point of intersection between neuroscience and AI and how this can contribute to the advancement of the understanding of intelligent behavior.	https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089315185&doi=10.3389\%2ffncom.2020.00063&partnerID=40&md5=a08d6811f61738bf6a0d4ec96eaac616	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Berthier2022	Infinite-Dimensional Sums-of-Squares for Optimal Control	In this paper, we introduce an approximation method to solve an optimal control problem via the Lagrange dual of its weak formulation, which applies to problems with an unknown, non-necessarily polynomial, dynamics accessed through samples, akin to model-free reinforcement learning. It is based on a sum-of-squares representation of the Hamiltonian, and extends a previous method from polynomial optimization to the generic case of smooth problems. Such a representation is infinite-dimensional and relies on a particular space of functions \endash a reproducing kernel Hilbert space \endash chosen to fit the structure of the control problem. After subsampling, it leads to a practical method that amounts to solving a semi-definite program. We illustrate our approach numerically on a low-dimensional control problem.	https://dx.doi.org/10.1109/CDC51059.2022.9992396	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bertsekas2022	Newton's method for reinforcement learning and model predictive control		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129617849&doi=10.1016\%2fj.rico.2022.100121&partnerID=40&md5=16006cc28238828a32db21a506214d3f	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhanumathi2017	A guide for the selection of routing protocols in WBAN for healthcare applications		https://doi.org/10.1186/s13673-017-0105-6	Excluded	conflict_resolution	E5: Other not a paper,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhatt2020	Software-Level Accuracy Using Stochastic Computing With Charge-Trap-Flash Based Weight Matrix	The in-memory computing paradigm with emerging memory devices has been recently shown to be a promising way to accelerate deep learning. Resistive processing unit (RPU) has been proposed to enable the vector-vector outer product in a crossbar array using a stochastic train of identical pulses to enable one-shot weight update, promising intense speed-up in matrix multiplication operations, which form the bulk of training neural networks. However, the performance of the system suffers if the device does not satisfy the condition of linear conductance change over around 1,000 conductance levels. This is a challenge for nanoscale memories. Recently, Charge Trap Flash (CTF) memory was shown to have a large number of levels before saturation, but variable non-linearity. In this paper, we explore the trade-off between the range of conductance change and linearity. We show, through simulations, that at an optimum choice of the range, our system performs nearly as well as the models trained using exact floating point operations, with less than 1\% reduction in the performance. Our system reaches an accuracy of 97.9\% on MNIST dataset, 89.1\% and 70.5\% accuracy on CIFAR-10 and CIFAR-100 datasets (using pre-extracted features). We also show its use in reinforcement learning, where it is used for value function approximation in Q-Learning, and learns to complete an episode the mountain car control problem in around 146 steps. Benchmarked to state-of-the-art, the CTF based RPU shows best in class performance to enable software equivalent performance.	https://dx.doi.org/10.1109/IJCNN48605.2020.9206631	Excluded	new_screen	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Bhattacharjee2022	Real-Time SIL Validation of a Novel PMSM Control Based on Deep Deterministic Policy Gradient Scheme for Electrified Vehicles	Vector control plays a critical role in a permanent magnet synchronous motor (PMSM) drive to deliver the desired torque in electrified vehicle applications. Motor speed and stator current control depend on various nonlinear motor parameters that influence the performance of PMSM. Moreover, tuning of speed and current controller parameters using conventional control techniques also depends on these PMSM parameters. To enhance the robustness of vector control and tracking methodology against PMSM parameter uncertainties and load disturbances, a novel deep reinforcement learning (DRL) based advanced speed and current control technique is proposed in this article. The proposed method mitigates the effects of disturbance due to parameter variations as well as the load torque. The novel architecture delivers closed-loop reinforcement learning agents trained with the deep deterministic policy gradient learning algorithm in the plant environment where the cost of exploration is expensive. First, an overview and need for the proposed DRL vector control architecture are provided. Subsequently, the design and training methods for the proposed DRL controller are elicited. Thereafter, the proposed control scheme is validated with real-time software-in-the-loop testing under various conditions and compared against adaptive proportional\endashintegral control of the same PMSM in OPAL-RT simulator.	https://dx.doi.org/10.1109/TPEL.2022.3153845	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhattacharyya2019	QFlow: A Reinforcement Learning Approach to High QoE Video Streaming over Wireless Networks		https://doi.org/10.1145/3323679.3326523	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhattacharyya2022	QFlow: A Learning Approach to High QoE Video Streaming at the Wireless Edge	The predominant use of wireless access networks is for media streaming applications. However, current access networks treat all packets identically, and lack the agility to determine which clients are most in need of service at a given time. Software reconfigurability of networking devices has seen wide adoption, and this in turn implies that agile control policies can be now instantiated on access networks. Exploiting such reconfigurability requires the design of a system that can enable a configuration, measure the impact on the application performance (Quality of Experience), and adaptively select a new configuration. Effectively, this feedback loop is a Markov Decision Process whose parameters are unknown. The goal of this work is to develop QFlow, a platform that instantiates this feedback loop, and instantiate a variety of control policies over it. We use the popular application of video streaming over YouTube as our use case. Our context is priority queueing, with the action space being that of determining which clients should be assigned to each queue at each decision period. We first develop policies based on model-based and model-free reinforcement learning. We then design an auction-based system under which clients place bids for priority service, as well as a more structured index-based policy. Through experiments, we show how these learning-based policies on QFlow are able to select the right clients for prioritization in a high-load scenario to outperform the best known solutions with over 25\% improvement in QoE, and a perfect QoE score of 5 over 85\% of the time.	https://dx.doi.org/10.1109/TNET.2021.3106675	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhonker2017	Playing SNEs in the retro learning environment			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bhutto2022	Reinforced Transformer Learning for VSI-DDoS Detection in Edge Clouds	Edge-driven software applications often deployed as online services in the cloud-to-edge continuum lack significant protection for services and infrastructures against emerging cyberattacks. Very-Short Intermittent Distributed Denial of Service (VSI-DDoS) attack is one of the biggest factors for diminishing the Quality of Services (QoS) and Quality of Experiences (QoE) for users on edge. Unlike conventional DDoS attacks, these attacks live for a very short time (on the order of a few milliseconds) in the traffic to deceive users with a legitimate service experience. To provide protection, we propose a novel and efficient approach for detecting VSI-DDoS attacks using reinforced transformer learning that mitigates the tail latency and service availability problems in edge clouds. In the presence of attacks, the users' demand for availing ultra-low latency and high throughput services deployed on the edge, can never be met. Moreover, these attacks send very-short intermittent requests towards the target services that enforce longer delays in users' responses. The assimilation of transformer with deep reinforcement learning accelerates detection performance under adverse conditions by adapting the dynamic and the most discernible patterns of attacks (e.g., multiplicative temporal dependency, attack dynamism). The extensive experiments with testbed and benchmark datasets demonstrate that the proposed approach is suitable, effective, and efficient for detecting VSI-DDoS attacks in edge clouds. The results outperform state-of-the-art methods with $0.9\\%-3.2\\%$ higher accuracy in both datasets.	https://dx.doi.org/10.1109/ACCESS.2022.3204812	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Biagioni2022	PowerGridworld: a framework for multi-agent reinforcement learning in power systems		https://doi.org/10.1145/3538637.3539616	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Biloki2019	Neural program planner for structured predictions			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Binxiang2019	A Deep Reinforcement Learning Malware Detection Method Based on PE Feature Distribution	Existing anti-virus software and malware detection methods, including signature-based and the machine learning-based malware detection methods, are unable to update the virus database in real time, resulting in poor resistance to malware variants. To solve this problem, this paper proposes a novel malware detection method based on deep reinforcement learning, which combines the advantages of Q-learning and neural network. Q-learning action selection strategy is adopted while solving the problem of high dimensional state space. Theoretical analysis and experimental results show that the proposed method can not only detect malware variants efficiently, but also perform well in many well-known anti-virus software, which is a new direction in the field of malware detection.	https://dx.doi.org/10.1109/ICISCE48695.2019.00014	Included	conflict_resolution		4
RL4SE	Bischoff2014	Policy search for learning robot control using sparse data	In many complex robot applications, such as grasping and manipulation, it is difficult to program desired task solutions beforehand, as robots are within an uncertain and dynamic environment. In such cases, learning tasks from experience can be a useful alternative. To obtain a sound learning and generalization performance, machine learning, especially, reinforcement learning, usually requires sufficient data. However, in cases where only little data is available for learning, due to system constraints and practical issues, reinforcement learning can act suboptimally. In this paper, we investigate how model-based reinforcement learning, in particular the probabilistic inference for learning control method (Pilco), can be tailored to cope with the case of sparse data to speed up learning. The basic idea is to include further prior knowledge into the learning process. As Pilco is built on the probabilistic Gaussian processes framework, additional system knowledge can be incorporated by defining appropriate prior distributions, e.g. a linear mean Gaussian prior. The resulting Pilco formulation remains in closed form and analytically tractable. The proposed approach is evaluated in simulation as well as on a physical robot, the Festo Robotino XT. For the robot evaluation, we employ the approach for learning an object pick-up task. The results show that by including prior knowledge, policy learning can be sped up in presence of sparse data.	https://dx.doi.org/10.1109/ICRA.2014.6907422	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Biswas2017	Machine learning for run-time energy optimisation in many-core systems	In recent years, the focus of computing has moved away from performance-centric serial computation to energy-efficient parallel computation. This necessitates run-time optimisation techniques to address the dynamic resource requirements of different applications on many-core architectures. In this paper, we report on intelligent run-time algorithms which have been experimentally validated for managing energy and application performance in many-core embedded system. The algorithms are underpinned by a cross-layer system approach where the hardware, system software and application layers work together to optimise the energy-performance trade-off. Algorithm development is motivated by the biological process of how a human brain (acting as an agent) interacts with the external environment (system) changing their respective states over time. This leads to a pay-off for the action taken, and the agent eventually learns to take the optimal/best decisions in future. In particular, our online approach uses a model-free reinforcement learning algorithm that suitably selects the appropriate voltage-frequency scaling based on workload prediction to meet the applications' performance requirements and achieve energy savings of up to 16\% in comparison to state-of-the-art-techniques, when tested on four ARM A15 cores of an ODROID-XU3 platform.	https://dx.doi.org/10.23919/DATE.2017.7927243	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Blau2011	INCENTIVES AND PERFORMANCE IN LARGE-SCALE LEAN SOFTWARE DEVELOPMENT An Agent-based Simulation Approach	The application of lean principles and agile project management techniques in the domain of large-scale software product development has gained tremendous momentum over the last decade. However, a simple transfer of good practices from the automotive industry combined with experiences from agile development on a team level is not possible due to fundamental differences stemming from the particular domain specifics - i.e. different types of products and components (material versus immaterial goods), knowledge work versus production systems as well as established business models. Especially team empowerment and the absence of a a hierarchical control on all levels impacts goal orientation and business optimization. In such settings, the design of adequate incentive schemes in order to align local optimization and opportunistic behavior with the overall strategy of the company is a crucial activity of central importance. Following an agent-based simulation approach with reinforcement learning, we (i) address the question of how information regarding backlog item dependencies is shared within and in between development teams on the product level subject to different incentive schemes. We (ii) compare different incentive schemes ranging from individual to team-based compensation. Based on our results, we are (iii) able to provide recommendations on how to design such incentives, what their effect is, and how to chose an adequate development structure to foster overall software product development flow by means of more economic decisions and thus resulting in a shorter time to market. For calibrating our simulation, we rely on practical experience from a very large software company piloting and implementing lean and agile for about three years.		Included	conflict_resolution		4
RL4SE	Blau2013	Steering through Incentives in Large-Scale Lean Software Development	The application of lean principles and agile project management techniques in the domain of large-scale software product development has gained tremendous momentum over the last decade. This results in empowerment of individuals which leads to increased flexibility but at the same time sacrifices managerial control through traditional steering practices. Hence, the design of adequate incentive schemes in order to align local optimization and opportunistic behavior with the overall strategy of the company is a crucial activity from a business perspective. Following an agent-based simulation approach with reinforcement learning, we (i) address the question of how information regarding backlog item dependencies is shared within and in between development teams on the product level subject to different incentive schemes. We (ii) compare different incentive schemes ranging from individual to team-based compensation. Based on our results, we are (iii) able to provide recommendations on how to design suitable incentive schemes in order to enable a goal-oriented steering of individual behavior in order to support the overall company objectives.		Included	new_screen		4
RL4SE	Bloem2015	Ground Delay Program Analytics with Behavioral Cloning and Inverse Reinforcement Learning	Historical data are used to build two types of models that predict Ground Delay Program implementation decisions and produce insights into how and why those decisions are made. More specifically, behavioral cloning and inverse reinforcement learning models are built that predict hourly Ground Delay Program implementation at Newark Liberty International and San Francisco International airports. Data available to the models include actual and scheduled air traffic metrics and observed and forecasted weather conditions. The developed random forest models are substantially better at predicting hourly Ground Delay Program implementation for these airports than the developed inverse reinforcement learning models. However, all of the models struggle to predict the initialization and cancellation of Ground Delay Programs. The structure of the models are also investigated in order to gain insights into Ground Delay Program implementation decision making. Notably, characteristics of both types of model suggest that Ground Delay Program implementation decisions are more tactical than strategic: they are made primarily based on conditions now or conditions anticipated in only the next couple of hours.	https://dx.doi.org/10.2514/1.I010304	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bloembergen2015	Evolutionary Dynamics of Multi-Agent Learning: A Survey	The interaction of multiple autonomous agents gives rise to highly dynamic and non-deterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.	https://dx.doi.org/10.1613/jair.4818	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bogaerts2020	Connecting the CoppeliaSim robotics simulator to virtual reality	The CoppeliaSim VR Toolbox provides a set of tools to experience CoppeliaSim robot simulation software in Virtual Reality and to return user interactions. Its primary focus is to create a platform that enables the fast prototyping and verification of robotic systems. Moreover, the generality of the toolbox ensures that it can be valuable in other contexts like robotics education, human-robot interaction or reinforcement learning. The software is designed to have a low entry threshold for moderately complex use cases, but can be extended to perform very complex visualizations for more experienced users. (c) 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	https://dx.doi.org/10.1016/j.softx.2020.100426	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bolcato2022	On the Value of Using 3D Shape and Electrostatic Similarities in Deep Generative Methods	Multiparameter optimization, the heart of drug design, isstill an open challenge. Thus, improved methods for automatedcompound design with multiple controlled properties are desired.Here, we present a significant extension to our previously describedfragment-based reinforcement learning method (DeepFMPO) for thegeneration of novel molecules with optimal properties. As before, thegenerative process outputs optimized molecules similar to the inputstructures, now with the improved feature of replacing parts of thesemolecules with fragments of similar three-dimensional (3D) shape and electrostatics. We developed and benchmarked a new pythonpackage, ESP-Sim, for the comparison of the electrostatic potential and the molecular shape, allowing the calculation of high-qualitypartial charges (e.g., RESP with B3LYP/6-31G**) obtained using the quantum chemistry program Psi4. By performing comparisonsof 3D fragments, we can simulate 3D properties while overcoming the notoriously difficult step of accurately describing bioactiveconformations. The new improved generative (DeepFMPO v3D) method is demonstrated with a scaffold-hopping exerciseidentifying CDK2 bioisosteres. The code is open-source and freely available	https://www.ncbi.nlm.nih.gov/pubmed/35271260	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bollinger2016	Multi-agent reinforcement learning for optimizing technology deployment in distributed multi-energy systems			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonarini1997	Anytime learning and adaptation of structured fussy behaviors	We present an approach to support effective learning and adaptation of behaviors for autonomous agents with reinforcement learning algorithms. These methods can identify control systems that optimize a reinforcement program, which is, usually, a straightforward representation of the designer's goals. Reinforcement learning algorithms usually are too slow to be applied in real time on embodied agents, although they provide a suitable way to represent the desired behavior. We have tackled three aspects of this problem: the speed of the algorithm, the learning procedure, and the control system architecture. The learning algorithm we have developed includes features to speed up learning, such as niche-based learning, and a representation of the control modules in terms of fuzzy rules that reduces the search space and improves robustness to noisy data. Our learning procedure exploits methodologies such as learning from easy missions and transfer of policy from simpler environments to the more complex. The architecture of our control system is layered and modular, so that each module has a low complexity and can be learned in a short time. The composition of the actions proposed by the modules is either learned or predefined. Finally, we adopt an anytime learning approach to improve the qualify of She control system on-line and to adapt it to dynamic environments. The experiments we present in this article concern learning to reach another moving agent in a real, dynamic environment that includes nontrivial situations such as that in which the moving target is faster than the agent and that in which the target is hidden by obstacles.	https://dx.doi.org/10.1177/105971239700500304	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonarini1997a	Anytime learning and adaptation of structured fuzzy behaviors		https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031287709&doi=10.1177\%2f105971239700500304&partnerID=40&md5=dbd83b523d7788d95252264c0473053b	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonarini2001	Learning fuzzy classifier systems for multi-agent coordination	We present ELF, a learning fuzzy classifier system (LFCS), and its application to the field of Learning Autonomous Agents. In particular, we will show how this kind of Reinforcement Learning systems can be successfully applied to learn both behaviors and their coordination for Autonomous Agents. We will discuss the importance of knowledge representation approach based on fuzzy sets to reduce the search space without losing the required precision. Moreover, we will show how we have applied ELF to learn the distributed coordination among agents which can exchange information with each other. The experimental validation has been done on software agents interacting in a real-time task. (C) 2001 Elsevier Science Inc. All rights reserved.	https://dx.doi.org/10.1016/S0020-0255(01)00149-9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonati2021	Intelligence and Learning in O-RAN for Data-Driven NextG Cellular Networks	Next generation (NextG) cellular networks will be natively cloud-based and built on programmable, virtualized, and disaggregated architectures. The separation of control functions from the hardware fabric and the introduction of standardized control interfaces will enable the definition of custom closed-control loops, which will ultimately enable embedded intelligence and real-time analytics, thus effectively realizing the vision of autonomous and self-optimizing networks. This article explores the disaggregated network architecture proposed by the O-RAN Alliance as a key enabler of NextG networks. Within this architectural context, we discuss the potential, the challenges, and the limitations of data-driven optimization approaches to network control over different timescales. We also present the first large-scale integration of O-RAN-compliant software components with an open source full-stack softwarized cellular network. Experiments conducted on Colosseum, the world's largest wireless network emulator, demonstrate closed-loop integration of real-time analytics and control through deep reinforcement learning agents. We also show the feasibility of radio access network (RAN) control through xApps running on the near-real-time RAN intelligent controller to optimize the scheduling policies of coexisting network slices, leveraging the O-RAN open interfaces to collect data at the edge of the network.	https://dx.doi.org/10.1109/MCOM.101.2001120	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonissone1995	Fuzzy controllers and fuzzy expert systems: Industrial applications of fuzzy technology		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079240074&doi=10.1117\%2f12.211793&partnerID=40&md5=541678530f35522b27485160cbbc2584	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bonissone1995a	Industrial Applications of Fuzzy Logic at General Electric		https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029276283&doi=10.1109\%2f5.364490&partnerID=40&md5=3f6ff5697ed0ed5c5aaa844f4885d0f2	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Bonsignorio2008	A comparison between under actuated and active bipedal locomotion gait control			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bosello2019	From Programming Agents to Educating Agents endash A Jason-Based Framework for Integrating Learning in the Development of Cognitive Agents		https://doi.org/10.1007/978-3-030-51417-4_9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bosello2020	From Programming Agents to Educating Agents - A Jason-Based Framework for Integrating Learning in the Development of Cognitive Agents	Recent advances and successes of machine learning techniques are paving the way to what is referred as Software 2.0 era and cognitive computing, in which traditional programming and software development is meant to be replaced by such techniques for many applications. If we consider agent-oriented programming, we believe that such developments trigger new interesting scenarios blending cognitive architecture such as the BDI one and techniques like Reinforcement Learning (RL) even more deeply compared to what has been proposed so far in the literature. In that perspective, we aim at exploring the integration of cognitive agent-oriented programming based on BDI with learning techniques so as to systematically exploit them in the agent development stage. The approach should support the design of BDI agents in which some plans can be explicitly programmed and others instead can be learned by the agent during the development/engineering stage. In that view, the development of an agent is metaphorically similar to an education process, in which first an agent is created with a set of basic programmed plans and then grow up in order to learn plans to achieve the goals for which the agent is meant to be designed. This paper presents and discusses this medium-term view, introducing a first model for a BDI agent programming framework integrating RL, a first implementation based on Jason programming language/platform and sketching a roadmap for this research line.	https://dx.doi.org/10.1007/978-3-030-51417-4_9	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bottinger2018	Deep Reinforcement Fuzzing	Fuzzing is the process of finding security vulnerabilities in input-processing code by repeatedly testing the code with modified inputs. In this paper, we formalize fuzzing as a reinforcement learning problem using the concept of Markov decision processes. This in turn allows us to apply state-of-the-art deep Q-learning algorithms that optimize rewards, which we define from runtime properties of the program under test. By observing the rewards caused by mutating with a specific set of actions performed on an initial program input, the fuzzing agent learns a policy that can next generate new higher-reward inputs. We have implemented this new approach, and preliminary empirical evidence shows that reinforcement fuzzing can outperform baseline random fuzzing.	https://dx.doi.org/10.1109/SPW.2018.00026	Included	new_screen		4
RL4SE	Boubin2022	MARbLE: Multi-Agent Reinforcement Learning at the Edge for Digital Agriculture	Digital agriculture, hailed as the fourth great agricultural revolution, employs software-driven autonomous agents for in-field crop management. Edge computing resources deployed near crop fields support autonomous agents with substantial computational needs for tasks such as AI inference. In large fields, using multiple autonomous agents, called swarms, can speed up crop management tasks if sufficient edge resources are provisioned. However, to use swarms today, farmers and software developers craft their own standalone solutions that are either simple and ineffective or complicated and hard-to-reproduce. We present MARbLE, a platform for developing and managing swarms. MARbLE provides an easy-to-use programming paradigm that helps users build swarm workloads using multi-agent reinforcement learning. Developers supply just two functions Map() and Eval(). The platform automatically compiles and deploys swarms and continuously updates the reinforcement learning models that govern their actions. Developers can experiment with multiple swarm and edge resource configurations both in simulation and with actual in-field runs. We studied real UAV swarms conducting digital agriculture missions. We observe that swarms demanded edge computing resources in bursts; the ratio of average to peak demand was 2.9X. MARbLE uses energy-saving load balancing policies to duty cycle machines during workload demand troughs, leveraging workload patterns to save edge energy. Using MARbLE, we found that four-agent swarms with load balancing techniques sped up missions by 2.1X and reduced edge energy usage by up to 2X compared to state of the art autonomous swarms.	https://dx.doi.org/10.1109/SEC54971.2022.00013	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Boubin2019	Autonomic Computing Challenges in Fully Autonomous Precision Agriculture	Precision agriculture examines crop fields, gathers data, analyzes crop health and informs field management. This data driven approach can reduce fertilizer runoff, prevent crop disease and increase yield. Frequent data collection improves outcomes, but also increases operating costs. Fully autonomous aerial systems (FAAS) can capture detailed images of crop fields without human intervention. They can reduce operating costs significantly. However, FAAS software must embed agricultural expertise to decide where to fly, which images to capture and when to land. This paper explores fully autonomous precision agriculture where FAAS map crop fields frequently. We have designed hardware and software architecture. We use unmanned aerial systems, edge computing components and software driven by reinforcement learning and ensemble models. In early results, we have collected data from an Ohio cornfield. We use this data to simulate a FAAS modeling crop yield. Our results (1) show that our approach predicts yield well and (2) can quantify computational demand. Computational costs can be prohibitive. We discuss how research on adaptive systems can reduce costs and enable fully autonomous precision agriculture. We also provide our simulation tools and dataset as part of our open source FAAS middleware, SoftwarePilot.	https://dx.doi.org/10.1109/ICAC.2019.00012	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bouzy2006	Monte-Carlo Go Reinforcement Learning Experiments	This paper describes experiments using reinforcement learning techniques to compute pattern urgencies used during simulations performed in a Monte-Carlo Go architecture. Currently, Monte-Carlo is a popular technique for computer Go. In a previous study, Monte-Carlo was associated with domain-dependent knowledge in the Go-playing program Indigo. In 2003, a 3times3 pattern database was built manually. This paper explores the possibility of using reinforcement learning to automatically tune the 3times3 pattern urgencies. On 9times9 boards, within the Monte-Carlo architecture of Indigo, the result obtained by our automatic learning experiments is better than the manual method by a 3-point margin on average, which is satisfactory. Although the current results are promising on 19times19 boards, obtaining strictly positive results with such a large size remains to be done	https://dx.doi.org/10.1109/CIG.2006.311699	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brandao2020	Decision support framework for the stock market using deep reinforcement learning	In stock markets, investors adopt different strategies to identify a sequence of profitable investment decisions to maximize their profits. To support the decision of investors, machine learning (ML) software is being applied. In particular, deep learning (DL) approaches are attractive since the stock market parameter presents a highly non-linear behavior, and since DL techniques can track short time and long time variations. In contrast to supervised ML techniques, deep reinforcement learning (DRL) gathers DL's benefits and adds the real-time adaptation and improvement of the machine learning model. In this paper, we propose a decision support framework for the stock market based on DRL. By learning the trading rules, our framework recognizes patterns, maximizes the profit obtained, and provides recommendations to the investors. The proposed DRL framework outperforms the state-of-the-art framework with 0.86 \% of F1 score for buy operations and 0.88 \% of F1 score for sale operations in terms of evaluating the positioning strategy.	https://dx.doi.org/10.1109/WCNPS50723.2020.9263712	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brandes2020	RF Waveform Synthesis Guided by Deep Reinforcement Learning	In this work, we demonstrate a system that enhances radio frequency (RF) fingerprints of individual transmitters via waveform modification to uniquely identify them amidst an ensemble of identical transmitters. This has the potential to enable secure identification, even in the presence of stolen and retransmitted unique device identifiers that are present in the transmitted waveforms, and ensures robust communications. This approach also lends itself to steganography as the waveform modifications can themselves encode information. Our system uses Bayesian program learning to learn specific characteristics of a set of emitters, and integrates the learned programs into a reinforcement learning architecture to build a policy for actions applied to the digital waveform before transmission. This allows the system to learn how to modify waveforms that leverage and emphasize inherent differences within RF front-ends to enhance their distinct characteristics while maintaining robust communications. In this ongoing research, we demonstrate our system in a small population, and provide a road map to expand it to larger populations that are expected in today's interconnected spaces.	https://dx.doi.org/10.1109/WIFS49906.2020.9360894	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brandt1995	Teaching literature searching in the context of the World Wide Web	As part of the required curriculum for medical students, we devised a literature-searching practicum that has been used for two years. In both years, we stressed going beyond the skills needed for using a particular searching program, towards a more conceptual approach to information searching. In the first year, the practicum was taught in a traditional lecture/hands-on format. In the second year, the lecture was replaced by a World Wide Web-based tutorial (http:@www.welch.jhu.edu/Education/tutorials/pra cticum.html). To our knowledge, this is the first Web-based resource intended to teach students about appropriate use of search technology. Comparison of student evaluations showed no difference in attitude toward the two versions of the practicum, and observation of student performance suggested similar levels of proficiency. We conclude that placing these educational materials on the Web (1) makes us practice what we preach; (2) is as effective as traditional teaching methods; and (3) gives students a resource for reinforcement learning.		Excluded	new_screen	E1: Does not define or use a RL method,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brassai2021	Intelligent agent based low level control of complex robotic systems	The use of intelligent agents, trained with reinforcement learning methods for control of complex mechanical systems, like humanoid robots has the potential to revolutionize the way we think about control problems. This way of learning is very similar to how we humans learn most of the things in our early age, thus proving really promising if we want to make robots able to learn tasks that require some form of intelligence. Throughout the research presented in this paper, a deep neural network based intelligent agent, with Actor-Critic architecture was trained with the Deep Deterministic Policy Gradient algorithm for the purpose of controlling a custom designed humanoid robot. For the training of the agent a simulation model of the physical robot is developed and integrated into a customizable simulated environment. The idea of low, actuator level control of complex systems by neural networks formulates the problem into a more abstract form while keeping the full control of the system without having to deal with the actual level of complexity. This can be further enhanced by expanding the abstraction from the software level to include some part of the hardware as well.	https://dx.doi.org/10.1109/INES52918.2021.9512904	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Brewer2020	Can Mindfulness Mechanistically Target Worry to Improve Sleep Disturbances? Theory and Study Protocol for App-Based Anxiety Program	Objective: Anxiety is associated with sleep disturbance and insomnia. Mindfulness-based interventions, such as mindfulness-based stress reduction, have shown consistent anxiety reduction. Mindfulness training has been theorized to affect reinforcement learning, affecting habitual behaviors such as smoking and overeating, but a direct mechanistic link between the use of mindfulness training for anxiety reduction and improvement in sleep has not been studied. Moreover, the mechanisms by which mindfulness might affect worry and subsequent sleep disturbances have not been elucidated. This study protocol evaluates the impact an app-based mindfulness training program for anxiety might have on decreasing worry and improvement in sleep. Method: A randomized controlled study will be conducted in approximately 80 adults with worry that interferes with their sleep. Participants will be randomly allocated (1:1) to two groups: treatment-as-usual (TAU) or TAU + App-Based Mindfulness Training (Unwinding Anxiety app). The primary outcomes will be the non-reactivity subscale of the Five Facet Mindfulness Questionnaire and Patient-Reported Outcomes Measurement Information System sleep quality measures (Baer et al., 2008; Yu et al., 2011). Secondary outcomes will include the Penn State Worry Questionnaire. Generalized Anxiety Disorder-7, and Multidimensional Assessment of Interoceptive Awareness Scale (Mehling et al., 2012; Meyer. Miller. Metzger, & Borkovec. 1990; Spitzer. Kroenke, Williams. & Lowe, 2006). Discussion: This study will be the first to test the mechanism of app-based mindfulness training on worry and sleep disturbance. Testing the mechanistic effects of mindfulness training using the science of behavior change framework will help move the field forward both in further elucidation of potential mechanisms of mindfulness (e.g., targeting reinforcement learning) and determining whether such a platform might be a viable method for delivering high-fidelity treatment at scale and for a low cost.	https://www.ncbi.nlm.nih.gov/pubmed/32833479	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Brien2015	Shapley Value Estimation for Compensation of Participants in Demand Response Programs	Designing fair compensation mechanisms for demand response (DR) is challenging. This paper models the problem in a game theoretic setting and designs a payment distribution mechanism based on the Shapley value (SV). As exact computation of the SV is in general intractable, we propose estimating it using a reinforcement learning algorithm that approximates optimal stratified sampling. We apply this algorithm to a DR program that utilizes the SV for payments and quantify the accuracy of the resulting estimates.	https://dx.doi.org/10.1109/TSG.2015.2402194	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bruns2020	Early verification of ISA extension specifications using deep reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091290343&doi=10.1145\%2f3386263.3406901&partnerID=40&md5=4d97bec747b4e08e91ac9a94d851d017	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Budyal2011	Unicast Quality of Service Routing in Mobile Ad Hoc Networks Based on Neuro-fuzzy Agents	This position paper presents Quality of Service (QoS) routing model in Mobile Ad hoc Networks (MANETs) by using software agents that employ fuzzy logic and neural networks for intelligent routing. It uses Dynamic Source Routing (DSR) in MANETs to find various paths and attributes. Fuzzy static agents decide whether each node on the path satisfies QoS requirement for multimedia application. The static neuro-fuzzy agents are used for training and learning to optimize the input and output fuzzy membership functions according to user requirement, and Q-learning (reinforcement learning) static agent is employed for fuzzy inference instead of experts experience. Mobile agents are used to maintain and repair the paths.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bujgoi2021	Intelligent Control of a Permanent Magnet DC Motor	This paper presents the use of artificial intelligence for the control of a permanent magnet DC motor. The system can be found especially in robotic systems that perform repetitive operations. The control law is generated by an intelligent Reinforcement Learning algorithm. From the numerous variants of this type of algorithm, the Policy Iteration type algorithm was chosen. The algorithm was experimentally implemented using a data acquisition system and Matlab/Simulink software. The control system was tested for several variants of the load (by changing its inertia moment). The simulation and experimental results show that the intelligent control method based on reinforcement learning has better trajectory tracking and vibrations suppression.	https://dx.doi.org/10.1109/ICCC51557.2021.9454643	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bulbul2022	Reinforcement Learning assisted Routing for Time Sensitive Networks	Recent developments in real-time critical systems pave the way for different application scenarios such as Industrial IoT with various quality-of-service (QoS) requirements. The most critical common feature of such applications is that they are sensitive to latency and jitter. Thus, it is desired to perform flow placements strategically considering application requirements due to limited resource availability. In this paper, path computation for time-sensitive networks is investigated while satisfying individual end-to-end delay requirements of critical traffic. The problem is formulated as a mixed-integer linear program (MILP) which is NP-hard with exponentially increasing computational complexity as the network size expands. To solve the MILP with high efficiency, we propose a reinforcement learning (RL) algorithm that learns the best routing policy by continuously interacting with the network environment. The proposed learning algorithm determines the variable action set at each decision-making state and captures different execution times of the actions. The reward function in the proposed algorithm is carefully designed for meeting individual flow deadlines. Simulation results indicate that the proposed reinforcement learning algorithm can produce near-optimal flow allocations (close by ~1.5 \%) and scales well even with large topology sizes.	https://dx.doi.org/10.1109/GLOBECOM48099.2022.10001630	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Bunel2018	Leveraging grammar and reinforcement learning for neural program synthesis			Included	new_screen		4
RL4SE	Cai2021	APPM: Adaptive Parallel Processing Mechanism for Service Function Chains	By replacing traditional hardware-based middleboxes with software-based Virtual Network Functions (VNFs) running on general-purpose servers, network function virtualization represents a promising technique to reduce the cost of service creation and increase the agility of network operations. Typically, Service Function Chains (SFCs) are adopted to orchestrate dynamical network services and facilitate management of network applications. Recently, SFC parallelism that implements parallel processing of VNFs has been investigated to further improve SFC service quality. However, the unreasonable service graph of parallel processing in existing parallelized SFCs (PSFCs) might cause excessive resource consumption; incoordination between PSFC deployment and scheduling also increases the queuing delay of VNFs and degrades PSFC performance. In this article, an adaptive parallel processing optimization mechanism (APPM) is proposed to self-adaptively adjust the service graph of PSFCs and intelligently solve the joint problem of PSFC deployment and scheduling. Specifically, APPM uses a parallelism optimization algorithm (POA) based on the bin packing problem with soft bin capacity to optimize the structure of the PSFC service graph. Afterward, APPM employs a joint optimization algorithm based on reinforcement learning (JORL) to jointly deploy and schedule the PSFCs optimized by POA via the online perception of environment status. Simulation results showed that POA reduces the SFC parallelism degree and resource consumption by about 35\%; JORL lowers SFC delay by reducing the queuing delay and has better overall performance than the state of the art algorithms even with limited resources.	https://dx.doi.org/10.1109/TNSM.2021.3052223	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cai2022	Multipath Routing for Traffic Engineering with Hypergraph Attention Enhanced Multi-Agent Reinforcement Learning	Traffic Engineering (TE) is an important mechanism of network performance optimization. Some traditional TE approaches mainly focus on static mapping of traffic flow to some available paths without considering the current network utilization or traffic load. Another class of TE methods is heuristic algorithm based on exact mathematical models, which has low adaptability in the face of complex and changing network environments. In this paper, we design a model-free and experience-driven TE method based on Reinforcement Learning (RL). In particular, we present a Multi-Agent RL (MARL) modelling for multipath routing with the objective to minimize the average flow completion time (FCT). Each router in the network is treated as an independent agent and decides how to split the flow originated from it across multiple paths. An efficient learning algorithm for MARL is developed. It employs the hypergraph attention mechanism to extract some relational representations of the network state by considering the graph nature of the routing paths. Moreover, it relies on the Software Defined Network (SDN) for centralized training, while it allows distributed execution without the need for a central controller. To evaluate our approach, comprehensive tests are conducted on two popular network topologies. The results show that 1) our approach significantly reduces FCT and provides better throughput compared to four baseline approaches, and 2) our approach is robust under various traffic load environments and the advantage over other methods is more significant in high traffic load environments.	https://dx.doi.org/10.1109/WOCC55104.2022.9880574	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cai2022a	A Robust and Learning Approach for Multi-Phase Aerial Search with UAVs		https://doi.org/10.1145/3503047.3503067	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cai2021a	MRDRL-ROS: A Multi Robot Deep Reinforcement Learning Platform based on Robot Operating System		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122433708&doi=10.1088\%2f1742-6596\%2f2113\%2f1\%2f012086&partnerID=40&md5=cecb9bf741a767904d312d86c7e95858	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Camara2015	A multi-agent system with reinforcement learning agents for biomedical text mining		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963548204&doi=10.1145\%2f2808719.2812596&partnerID=40&md5=b0f53ffb31f9502e8c0f28fc918125fd	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Candelieri2019	Business Information Systems for the Cost/Energy Management of Water Distribution Networks: A Critical Appraisal of Alternative Optimization Strategies	The objective of this paper is to show how smart water networks enable new strategies for the energy cost management of the network, more precisely Pump Scheduling Optimization. This problem is traditionally solved using mathematical programming and, more recently, nature inspired meta-heuristics. The schedules obtained by these methods are typically not robust both respect to random variations in the water demand and the non-linear features of the model. The authors consider three alternative optimization strategies: (i) global optimization of black-box functions, based on a Gaussian model and the use of the hydraulic simulator (EPANET) to evaluate the objective function; (ii) Multi Stage Stochastic Programming, which models the stochastic evolution of the water demand through a scenario analysis to solve an equivalent large scale linear program; and finally (iii), Approximate Dynamic Programming, also known as Reinforcement Learning. With reference to real life experimentation, the last two strategies offer more modeling flexibility, are demand responsive and typically result in more robust solutions (i.e. pump schedules) than mathematical programming. More specifically, Approximate Dynamic Programming works on minimal modelling assumption and can effectively leverage on line data availability into robust on-line Pump Scheduling Optimization.	https://dx.doi.org/10.1007/978-3-030-04849-5_1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Candrawati2016	Adaptive approach in handling human inactivity in computer power management			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Canino2018	Stochastic energy optimization for mobile GPS applications		https://doi.org/10.1145/3236024.3236076	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cao2001	An application of GPS systems on a mobile robot	The purpose of this paper is to describe the use of Global Positioning Systems (GPS) as geographic information and navigational system for a ground based mobile robot. Several low cost wireless systems are now available for a variety of innovative automobile applications including location, messaging and tracking and security. Experiments were conducted with a test bed mobile robot, Bearcat II, for point-to-point motion using a Motorola GPS in June 2001. The Motorola M12 Oncore GPS system is connected to the Bearcat U main control computer through a RS232 interface. A mapping program is used to define a desired route. Then GPS information may be displayed for verification. However, the GPS information is also used to update the control points of the mobile robot using a reinforcement learning method. Local position updates are also used when found in the environment. The significance of the method is in extending the use of GPS to local vehicle control that requires more resolution that is available from the raw data using the adaptive control method.	https://dx.doi.org/10.1117/12.444198	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cao2022	Ensemble Approaches for Test Case Prioritization in UI Testing		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137162453&doi=10.18293\%2fSEKE2022-148&partnerID=40&md5=3b9a5bf69ec1ae27c7d0f606fd7f08d9	Excluded	conflict_resolution	E1: Does not define or use a RL method,E1: Does not define or use a RL method	4
RL4SE	Cao2021	Automatic HMI Structure Exploration Via Curiosity-Based Reinforcement Learning	Discovering the underlying structure of HMI software efficiently and sufficiently for the purpose of testing without any prior knowledge on the software logic remains a difficult problem. The key challenge lies in the complexity of the HMI software and the high variance in the coverage of current methods. In this paper, we introduce the PathFinder, an effective and automatic HMI software exploration framework. PathFinder adopts a curiosity-based reinforcement learning framework to choose actions that lead to the discovery of more unknown states. Additionally, PathFinder progressively builds a navigation model during the exploration to further improve state coverage. We have conducted experiments on both simulations and real-world HMI software testing environment, which comprise a full tool chain of automobile dashboard instrument cluster. The exploration coverage outperforms manual and fuzzing methods which are the current industrial standards.	https://dx.doi.org/10.1109/ASE51524.2021.9678703	Included	conflict_resolution		4
RL4SE	Cardellini2022	irs-partition: An Intrusion Response System utilizing Deep Q-Networks and system partitions	Intrusion Response is a relatively new field of research. Recent approaches for the creation of Intrusion Response Systems (IRSs) use Reinforcement Learning (RL) as a primary technique for the optimal or near-optimal selection of the proper countermeasure to take in order to stop or mitigate an ongoing attack. However, most of them do not consider the fact that systems can change over time or, in other words, that systems exhibit non-stationary behaviors. Furthermore, stateful approaches, such as those based on RL, suffer from the curse of dimensionality, due to the state space growing exponentially with the size of the protected system. In this paper, we introduce and develop an IRS software prototype, named irs-partition. It leverages the partitioning of the protected system and Deep Q-Networks to address the curse of dimensionality by supporting a multi-agent formulation. Furthermore, it exploits transfer learning to follow the evolution of non-stationary systems. ?? 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).	https://dx.doi.org/10.1016/j.softx.2022.101120	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cardoso2000	Using and Evaluating Adaptive Agents for Electronic Commerce Negotiation			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cardoso1999	A Multi-agent System for Electronic Commerce including Adaptive Strategic Behaviours			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cardozo2022	Next Generation Context-oriented Programming: Embracing Dynamic Generation of Adaptations	Context-oriented Programming (COP) first appeared in 2005 as a way to enable the dynamic adaptation of software systems to specific situations in their surrounding environment. Multiple COP languages have since been proposed, and used in numerous adaptive systems areas, enabling dynamic swapping and composition of adaptive behavior at run-time. However, until recently, all approaches relied on the offline pre-definition of adaptive behavior, limiting the adaptations to only those foreseen at design time. Auto-COP recently emerged as an approach to shift adaptation definition to run-time, if and when the need for adaptations to new contexts arises, by utilizing reinforcement learning techniques. In this paper, we use Auto-COP as a starting point to discuss the research path to achieve a completely dynamic adaptive system. We discuss the potential benefits of such an automated Al-based approach, present several application domain categories where dynamic adaptation definition would enable adaptivity breakthroughs, and discuss open challenges in developing such a fully automated approach.	https://dx.doi.org/10.5381/jot.2022.21.2.a5	Excluded	conflict_resolution	E3: Only conceptual results are reported,E3: Only conceptual results are reported	4
RL4SE	Cardozo2017	Peace COrP: learning to solve conflicts between contexts		https://doi.org/10.1145/3117802.3117803	Included	conflict_resolution		4
RL4SE	Carfora2020	Dialogue management in conversational agents through psychology of persuasion and machine learning		https://doi.org/10.1007/s11042-020-09178-w	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Carlucho2019	Double Q-PID algorithm for mobile robot control		https://doi.org/10.1016/j.eswa.2019.06.066	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Caro2022	AI-as-a-Service Toolkit for Human-Centered Intelligence in Autonomous Driving	This paper presents a proof-of-concept implementation of the AI-as-a-Service toolkit developed within the H2020 TEACHING project and designed to implement an autonomous driving personalization system according to the output of an automatic driver's stress recognition algorithm, both of them realizing a Cyber-Physical System of Systems. In addition, we implemented a data-gathering subsystem to collect data from different sensors, i.e., wearables and cameras, to automatize stress recognition. The system was attached for testing to a driving emulation software, CARLA, which allows testing the approach's feasibility with minimum cost and without putting at risk drivers and passengers. At the core of the relative subsystems, different learning algorithms were implemented using Deep Neural Networks, Recurrent Neural Networks, and Reinforcement Learning.	https://dx.doi.org/10.1109/PerComWorkshops53856.2022.9767501	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Carvalho2012	An adaptive multi-agent-based approach to smart grids control and optimization		https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857620276&doi=10.1007\%2fs12667-012-0054-0&partnerID=40&md5=7c73763863269e6ab4c80f8f0e5fe4da	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cazenave2022	Mobile Networks for Computer Go		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097440578&doi=10.1109\%2fTG.2020.3041375&partnerID=40&md5=14f27c19f85ef7f9bba980a6f8e519ca	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Celik2019	Chance-Constrained Trajectory Optimization for Non-linear Systems with Unknown Stochastic Dynamics	Iterative trajectory optimization techniques for non-linear dynamical systems are among the most powerful and sample-efficient methods of model-based reinforcement learning and approximate optimal control. By leveraging time-variant local linear-quadratic approximations of system dynamics and reward, such methods can find both a target-optimal trajectory and time-variant optimal feedback controllers. However, the local linear-quadratic assumptions are a major source of optimization bias that leads to catastrophic greedy updates, raising the issue of proper regularization. Moreover, the approximate models' disregard for any physical state-action limits of the system causes further aggravation of the problem, as the optimization moves towards unreachable areas of the state-action space. In this paper, we address the issue of constrained systems in the scenario of online-fitted stochastic linear dynamics. We propose modeling state and action physical limits as probabilistic chance constraints linear in both state and action and introduce a new trajectory optimization technique that integrates these probabilistic constraints by optimizing a relaxed quadratic program. Our empirical evaluations show a significant improvement in learning robustness, which enables our approach to perform more effective updates and avoid premature convergence observed in state-of-the-art algorithms.	https://dx.doi.org/10.1109/IROS40897.2019.8967794	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Cen2013	Reinforcement learning in information searching			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chakole2021	A Q-learning agent for automated trading in equity stock markets	Trading strategies play a vital role in Algorithmic trading, a computer program that takes and executes automated trading decisions in the stock market. The conventional wisdom is that the same trading strategy is not profitable for all stocks all the time. The selection of a trading strategy for the stock at a particular time instant is the major research problem in the stock market trading. An optimal dynamic trading strategy generated from the current pattern of the stock price trend can attempt to solve this problem. Reinforcement Learning can find this optimal dynamic trading strategy by interacting with the actual stock market as its environment. The representation of the state of the environment is crucial for performance. We have proposed two different ways to represent the discrete states of the environment. In this work, we trained the trading agent using the Q-learning algorithm of Reinforcement Learning to find optimal dynamic trading strategies. We experimented with the two proposed models on real stock market data from the Indian and American stock markets. The proposed models outperformed the Buy-and-Hold and Decision-Tree based trading strategy in terms of profitability. (C) 2020 Elsevier Ltd. All rights reserved.	https://dx.doi.org/10.1016/j.eswa.2020.113761	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chalita2016	Reinforcement learning in a bio-connectionist model based in the thalamo-cortical neural circuit	In a previous study, we presented a program to simulate a particular dynamic of the thalamocortical biological system. The method used was called bio-connectionism which linked the thalanno-cortical mechanism reproduced with animal perception. In this presentation, a reinforcement learning program is supported by this mechanism. In a game world designed to test the model developed, the agent is assigned to a character that must learn by trial and error from its own experience upon recognition of aversive and appetitive patterns. The results confirm, support and extend the notion of configuration, a term familiar with sparse coding principles. If, as it is documented, this mechanism observed in sensory areas can be thought as condition of perception, the brain areas taken together each in its interaction with a respective sub-thalamic nucleus are suspected to be considered as condition of cognition. We introduce some philosophical questions derived from the experimental results in the discussion section. (C) 2016 Elsevier B.V. All rights reserved.	https://dx.doi.org/10.1016/j.bica.2016.03.001	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chan2019	Autonomous Imaging and Mapping of Small Bodies Using Deep Reinforcement Learning	The mapping and navigation around small unknown bodies continues to be an extremely interesting and exciting problem in the area of space exploration. Traditionally, the spacecraft trajectory for mapping missions is designed by human experts using hundreds of hours of human time to supervise the navigation and orbit selection process. While the current methodology has performed adequately for previous missions (such as Rosetta, Hayabusa and Deep Space), as the demands for mapping missions expand, additional autonomy during the mapping and navigation process will become necessary for mapping spacecraft. In this work we provide the framework for optimizing the autonomous imaging and mapping problem as a Partially Observable Markov Decision Process (POMDP). In addition, we introduce a new simulation environment which simulates the orbital mapping of small bodies and demonstrate that policies trained with our POMDP formulation are able to maximize map quality while autonomously selecting orbits and supervising imaging tasks. We conclude with a discussion of integrating Deep Reinforcement Learning modules with classic flight software systems, and some of the challenges that could be encountered when using Deep RL in flight-ready systems.	https://dx.doi.org/10.1109/AERO.2019.8742147	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chandrasekar2022	Hybrid Deep Learning Approach for Improved Network Connectivity in Wireless Sensor Networks	Wireless sensor networks occupy a prominent role in industrial as well as scientific applications. Lifetime enhancement and coverage are the major factors considered while designing the network. Various research models are evolved by considering the scheduling and routing process to solve the network lifetime issues. However, coverage and connectivity is another important factor that affects the lifetime of the remaining nodes. When a large number of sensors are deployed randomly, scheduling is preferred to enhance the network lifetime, but it leads to coverage issues. Other than scheduling, node damage, battery exhaustion, software and hardware failures might lead to coverage issues. Preserving the network connectivity while maximizing the network coverage is a crucial task in wireless sensor networks. To preserve the network connectivity and improve the wireless sensor networks coverage this research work presents a hybrid deep learning approach using a deep neural network and reinforcement learning algorithm. The Proposed model is experimentally verified and compared with conventional deep neural network and reinforcement learning algorithms to demonstrate the better balancing characteristics between network coverage and lifetime.	https://dx.doi.org/10.1007/s11277-022-10052-1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chang2021	Accuracy vs. Efficiency: Achieving both Through Hardware-Aware Quantization and Reconfigurable Architecture with Mixed Precision	We propose a hardware/software co-design framework, which leverages hardware-aware quantization and a reconfigurable processor to improve the computational efficiency of convolutional neural networks (CNNs) on tiny IoT devices based on reconfigurable platforms. Firstly, we proposed a multi-objective optimization value function that can weigh accuracy, the size of CNN models, and computational delay, to improve the efficiency of the mixed- precision quantization algorithm based on deep reinforcement learning. Secondly, we propose a reconfigurable CNN processor that can adapt to the computing characteristics of various quantized CNN models, as well as a reconfigurable computing array and an on-chip elastic buffer, to improve the performance and computing efficiency on edge equipment. Finally, we demonstrate the effectiveness of the proposed co-design method through an extensive evaluation of the Ultra96-V2 platform. With respect to the well-known CNNs\emdashVGG-16, ResNet-50, and MobileNet-V2, the experimental result shows that the throughput of 216.6 GOPS, 214.0 GOPS, and 53.6 GOPS, the computing efficiency of 0.63GOPS/DSP, 0.64GOPS/DSP, and 0.24 GOPS/DSP, respectively. In addition, achieving a better optimized trade-off between the computing efficiency and accuracy compared with the recently proposed CNN processor with fixed bit-width and mixed-precision.	https://dx.doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00033	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chao2015	November 2015: This Month in JoVE	Here's a look at what's coming up in the November 2015 issue of JoVE: The Journal of Visualized Experiments. In JoVE Neuroscience, we know that fruit flies (Drosophila melanogaster) are a lot like humans in many ways-especially because they like their personal space. And in fruit flies, this preferred social distance can be measured using the social space assay. McNiel et al. demonstrate this straightforward protocol, which requires only simple equipment and experimental setups. Flies are blown into a social chamber and forced to form a tight group. Then they're allowed to take their preferred distance from one another. These distances are measured and processed with free online software (ImageJ). This social space assay provides a simple yet powerful paradigm for analyzing the underlying neurogenetics and environmental factors of social behavior. In JoVE Behavior, humans have a natural ability to acquire new motor skills, and this ability is crucial for upper limb amputees as they learn the complex control schemes for advanced multifunctional prosthetics. This month, Roche et al. present a case study of a structured rehabilitation method, which aims to improve multifunctional prosthetic control. Their subject underwent a structured protocol of imitation, repetition, and reinforcement learning. The subject demonstrated improvement in a widely used hand function test. This study suggests that a structured rehabilitation method may facilitate proficiency for multifunctional prosthetic control, and provides basis for larger clinical studies. Stress is a major concept in JoVE Behavior, and comprises various physiological responses to challenges. Among other responses, stress increases body temperature, which provides a quantitative measure of this response. However, the very act of measuring body temperature can be stressful to subjects, especially if they're wild animals. So Jerem et al. present a protocol for noninvasively measuring temperature in wild birds using infrared thermography. Their set-up is equipped with bird food and an infrared camera. This takes a thermal video of the bird before and after the researcher remotely closes the box, which acts as a mild acute stressor. The skin around the bird's eye is the warmest area in the image, and this protocol provides a time series of eye-region temperature with fine temporal resolution. With further validation, this method may prove valuable for studying the dynamics of the stress response for a wide range of researchers from environmental science to medicine. You've just had a sneak peek of the November 2015 issue of JoVE. Visit the website to see the full-length articles, plus many more, in JoVE: The Journal of Visualized Experiments.	https://dx.doi.org/10.3791/5758	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E5: Other not a paper	4
RL4SE	Chellaswamy2022	6-phase DFIG for wind energy conversion system: A hybrid approach	This research presents a novel wind power system based on a six-phase doubly-fed induction generator (DFIG). Optimization approaches are required to improve the efficiency of the traditional controllers. This study introduces a blended method for DFIG-based wind power transformation systems that combines quantum process and deep reinforcement learning (QPDRL) to improve control efficiency. It will be driven by using online control algorithms to eliminate the optimizing step and upgrade online control strategies. The proposed QPDRL can prevent local optimum solutions, forecast the future essential phase, and update DFIG-based wind power plants' regulation methods online. For two distinct scenarios, the QPDRL was contrasted with the proportional integral derivative (PID) controller, fractional-order PID, and reinforcement learning (for changeable air velocity, there are two types of arbitrary and step amplitudes). Matlab software was used to experiment. As air velocity variations exist, the findings revealed a 62\% reduction in the DC link voltage ripples and a 99\% reduction in speed overshoot with wind velocities overrun. Finally, comparing PID controls revealed a 42.15 percent reduction in grid current THD and an 11.38 percent reduction in the generator current.	https://dx.doi.org/10.1016/j.seta.2022.102497	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2020	Reinforcement Learning on Computational Resource Allocation of Cloud-based Wireless Networks	Wireless networks used for Internet of Things (IoT) are expected to largely involve cloud-based computing and processing. Softwarised and centralised signal processing and network switching in the cloud enables flexible network control and management. In a cloud environment, dynamic computational resource allocation is essential to save energy while maintaining the performance of the processes. The stochastic features of the Central Processing Unit (CPU) load variation as well as the possible complex parallelisation situations of the cloud processes makes the dynamic resource allocation an interesting research challenge. This paper models this dynamic computational resource allocation problem into a Markov Decision Process (MDP) and designs a model-based reinforcement-learning agent to optimise the dynamic resource allocation of the CPU usage. Value iteration method is used for the reinforcement-learning agent to pick up the optimal policy during the MDP. To evaluate our performance we analyse two types of processes that can be used in the cloud-based IoT networks with different levels of parallelisation capabilities, i.e., Software-Defined Radio (SDR) and Software-Defined Networking (SDN). The results show that our agent rapidly converges to the optimal policy, stably performs in different parameter settings, outperforms or at least equally performs compared to a baseline algorithm in energy savings for different scenarios.	https://dx.doi.org/10.1109/WF-IoT48130.2020.9221234	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021	Grey-box Fuzzing With Deep Reinforcement Learning And Process Trace Back	Grey-box fuzzing, as a software testing technique, can find possible program bugs such as memory leaks, assertion failures and invalid input by generate random data then input it into a program, and monitor program exceptions. With program running, grey-box fuzzing collected the branch information in order to guide choosing next seeds. In this paper, we try to use the concept of Markov decision processes to formalize grey-box fuzzing as a deep reinforcement learning problem, and use process trace back(Intel Process Trace) to collect branch information in order to improve its efficiency toward binary programs. The experiments show this approach can outperform baseline random fuzzing and gain performance improvement.	https://dx.doi.org/10.1109/AEMCSE51986.2021.00238	Included	new_screen		4
RL4SE	Chen2019	Reinforcement-Learning-Based Test Program Generation for Software-Based Self-Test	Software-based Self-test (SBST) has been recognized as a promising complement to scan-based structural Built-in Self-test (BIST), especially for in-field self-test applications. In response to the ever-increasing complexities of the modern CPU designs, machine learning algorithms have been proposed to extract processor behavior from simulation data and help constrain ATPG to generate functionally-compatible patterns. However, these simulation-based approaches in general suffer sample inefficiency, i.e., only a small portion of the simulation traces are relevant to fault detection. Inspired by the recent advances in reinforcement learning (RL), we propose an RL-based test program generation technique for transition delay fault (TDF) detection. During the training process, knowledge learned from the simulation data is employed to tune the simulation policy; this close-loop approach significantly improves data efficiency, compared to previous open-loop approaches. Furthermore, RL is capable of dealing with delayed responses, which is common when executing processor instructions. Using the trained RL model, instruction sequences that bring the processor to the fault-sensitizing states, i.e., TDF test patterns, can be generated. The proposed test program generation technique is applied to a MIPS32 processor. For TDF, the fault coverage is 94.94\%, which is just 2.57\% less than the full-scan based approach.	https://dx.doi.org/10.1109/ATS47505.2019.00013	Included	new_screen		4
RL4SE	Chen2019a	Reinforcement-Learning Based Test Program Generation for Software-Based Self-Test	Software-based Self-test (SBST) has been recognized as a promising complement to scan-based structural Built-in Self-test (BIST), especially for in-field self-test applications. In response to the ever-increasing complexities of the modern CPU designs, machine learning algorithms have been proposed to extract processor behavior from simulation data and help constrain ATPG to generate functionally-compatible patterns. However, these simulation-based approaches in general suffer sample inefficiency, i.e., only a small portion of the simulation traces are relevant to fault detection. Inspired by the recent advances in reinforcement learning (RL), we propose an RL-based test program generation technique for transition delay fault (TDF) detection. During the training process, knowledge learned from the simulation data is employed to tune the simulation policy; this close-loop approach significantly improves data efficiency, compared to previous open-loop approaches. Furthermore, RL is capable of dealing with delayed responses, which is common when executing processor instructions. Using the trained RL model, instruction sequences that bring the processor to the fault-sensitizing states, i.e., TDF test patterns, can be generated. The proposed test program generation technique is applied to a MIPS32 processor. For TDF, the fault coverage is 94.94\%, which is just 2.57\% less than the full-scan based approach.	https://dx.doi.org/10.1109/ATS47505.2019.00013	Excluded	conflict_resolution	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021a	Traffic flow of connected and automated vehicles in Smart Cities: Human-Centric	This paper proposes a novel Connected and Automated Vehicle (CAV) model as a scanner for heterogeneous traffic flows, which employs CAV on the road to detect traffic flow characteristics in multiple traffic scenes through various sensors. The model contains the hardware platform and software algorithm of CAV, and the analysis of traffic flow detection and simulation by Flow Project from Mobile Sensing Lab at UC Berkeley and Amazon AWS Machine Learning research grants based on SUMO, where the driving of the car is mainly controlled by Reinforcement Learning (RL). The simulation results showed that the traffic flow scanning, tracking and data recording by CAV are continuous and effective for the wide range and identification confirm function when CAV are in one lane. The effective detection area of CAV is in bow shape in the heterogeneous traffic flow, the occlusion rate is not associated with the lane position of CAV. Therefore, the calculated results should be filtered and optimized to enhance the confidence of heterogeneous traffic data collected. Currently, standards or most practitioners are not aware of this.	https://dx.doi.org/10.1109/SWC50871.2021.00049	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022	The Scanner of Heterogeneous Traffic Flow in Smart Cities by an Updating Model of Connected and Automated Vehicles	The problems of traditional traffic flow detection and calculation methods include limited traffic scenes, high system costs, and lower efficiency over detecting and calculating. Therefore, in this paper, we presented the updating Connected and Automated Vehicles (CAVs) model as the scanner of heterogeneous traffic flow, which uses various sensors to detect the characteristics of traffic flow in several traffic scenes on the roads. The model contains the hardware platform, software algorithm of CAV, and the analysis of traffic flow detection and simulation by Flow Project, where the driving of vehicles is mainly controlled by Reinforcement Learning (RL). Finally, the effectiveness of the proposed model and the corresponding swarm intelligence strategy is evaluated through simulation experiments. The results showed that the traffic flow scanning, tracking, and data recording performed continuously by CAVs are effective. The increase in the penetration rate of CAVs in the overall traffic flow has a significant effect on vehicle detection and identification. In addition, the vehicle occlusion rate is independent of the CAV lane position in all cases. The complete street scanner is a new technology that realizes the perception of the human settlement environment with the help of the Internet of Vehicles based on 5G communications and sensors. Although there are some shortcomings in the experiment, it still provides an experimental reference for the development of smart vehicles.	https://dx.doi.org/10.1109/TITS.2022.3165155	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018	The Research on Adaptive Reinforcement Learning Technique Based on Convex Polyhedra Abstraction Domain		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048118328&doi=10.11897\%2fSP.J.1016.2018.00112&partnerID=40&md5=9344f17a7113afbe67927bc811137d33	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022a	LAMP: Load-Balanced Multipath Parallel Transmission in Point-to-Point NoCs	Network-on-chip (NoC) is an emerging paradigm that is able to connect a significant amount of processing elements (PEs). However, as a distributed subsystem, NoC resources have not been exploited to the fullest. Multipath parallel transmission, which splits one message into multiple parts and sends them simultaneously, shows its efficiency in utilizing NoC resources and further reducing the transmission latency. However, this method is not fully optimized in previous works, especially for emerging point-to-point NoCs due to the following reasons: 1) only limited shortest paths are chosen; 2) static message splitting strategy without considering NoC utilization state increases contentions; and 3) the optimization of hardware that supports multipath parallel transmission is missing, resulting in additional overheads. Thus, we propose LAMP, a software and hardware collaborated design to efficiently utilize resources and reduce latency in point-to-point NoCs through the load-balanced multipath parallel transmission. Specifically, we propose a reinforcement learning-based algorithm to decide when and how to split messages, and which path should be used according to traffic loads. Also, the temporal and spatial load-balancing algorithms are proposed so that the message size is adjusted properly to utilize NoC resources. Moreover, we revise the hardware design to support multipath parallel transmission efficiently. Extensive experiments show that our algorithm achieves a remarkable performance improvement (+18.0\% to +29.9\%) when compared with the state-of-the-art dual-path algorithm. Our hardware design decreases power and area consumption by 23.2\% and 10.3\% over the dual-path hardware.	https://dx.doi.org/10.1109/TCAD.2022.3151021	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022b	Neurotrie: Deep Reinforcement Learning-based Fast Software IPv6 Lookup	IPv6 has shown notable growth in recent years, imposing the need for high-speed IPv6 lookup. As the forwarding rate of virtual switches continues increasing, software-based IPv6 lookup without using special hardware such as TCAM, GPU, and FPGA is of academic interest and industrial importance. Existing studies achieve fast software IPv4 lookup by reducing the operation number, as well as reducing the memory footprint so as to benefit from CPU cache. However, in the situation of 128-bit IPv6 addresses, it is challenging to keep both operation numbers and memory footprints small. To address the issue, we propose the Neurotrie data structure, which supports fast lookup and arbitrary strides. Thus, a good balance can be made between trie depth and memory footprint by computing the proper stride for each Neurotrie node. We model the optimal Neurotrie problem which minimizes the depth with limited memory footprint and develop a pseudo-polynomial time baseline algorithm to construct Neurotrie using dynamic programming. To improve the performance and reduce the computation complexity, we develop a deep reinforcement learning-based approach, which leverages a deep neural network to construct Neurotrie efficiently, based on characteristics captured from real IPv6 prefixes. We further refine the data structure and develop an efficient mechanism for routing updates. Experiments on real routing tables show that Neurotrie achieves a lookup rate 34\% higher than that of state-of-the-art approaches.	https://dx.doi.org/10.1109/ICDCS54860.2022.00093	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021b	Parallel Multipath Transmission for Burst Traffic Optimization in Point-to-Point NoCs		https://doi.org/10.1145/3453688.3461521	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018a	A Q-learning-based network content caching method	Cloud computing provides users with a distributed computing environment offering on-demand services. As its technologies become gradually mature and its application becomes more universal, cloud computing greatly reduces users' costs while increasing working efficiency of enterprises and individuals (Futur Gener Comput Syst 25:599-616, 2009). Software as a service (SaaS), as a kind of information servicing model based on cloud platforms, is rising with the developments of Internet technologies and the maturing of application software. The responsibility of a SaaS server is to timely and accurately satisfy users' needs for information. An intelligent and efficient content caching solution or method plays a vital role in that. This paper proposes a reinforcement learning (RL)-based content caching method named time-based Q Cacher (TQC) which effectively solves the problem of low hit ratio of server caching and ultimately achieves an intelligent, flexible, and highly adaptable content caching model.	https://dx.doi.org/10.1186/s13638-018-1268-1	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2020a	QMORA: A Q-Learning based Multi-objective Resource Allocation Scheme for NFV Orchestration	To satisfy the various quality-of-service (QoS) re-quirements with minimum network costs, network functions virtualization (NFV) is proposed as an emerging wireless architecture that migrates network functions from dedicated hardware appliances to software instances running in virtual computing platforms. One crucial issue in NFV is to solve the orchestration of virtualized network functions (VNFs) to reduce costs and to improve the management flexibility of telecommunications service providers (TSPs). Particular, multiple objectives are required to be considered for orchestrating VNFs in order to achieve overall system performance. This can be optimally solved in small scale using integer linear programming (ILP) algorithms with high accuracy but low time efficiency. On the other hand, heuristic algorithms can be applied for solving part of the objectives in NFV resource allocation with high time efficiency but low accuracy. To tackle the above challenges, QMORA, a ${Q}$-learning based multi-objective resource allocation approach, is proposed to solve multi-objective optimization in NFV orchestration (NFVO) efficiently and accurately. Particularly, the approach includes reinforcement learning module and VNFs placement module. Reinforcement learning module is responsible for generating the ``best'' candidate paths. VNFs placement module is responsible for selecting optimal nodes on the generated candidate paths to host VNFs required for flows. The simulation results in the real ISP topology show that the proposed QMORA can balance the multi-objective including maximizing number of flows admitted to the network, minimizing path stretch, balancing the load among VNF instances and minimizing link occupation rate compared with other heuristic approaches.	https://dx.doi.org/10.1109/VTC2020-Spring48590.2020.9128963	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2020b	Enhanced Compiler Bug Isolation via Memoized Search	Compiler bugs can be disastrous since they could affect all the software systems built on the buggy compilers. Meanwhile, diagnosing compiler bugs is extremely challenging since usually limited debugging information is available and a large number of compiler files can be suspicious. More specifically, when compiling a given bug-triggering test program, hundreds of compiler files are usually involved, and can all be treated as suspicious buggy files. To facilitate compiler debugging, in this paper we propose the first reinforcement compiler bug isolation approach via structural mutation, called RecBi. For a given bug-triggering test program, RecBi first augments traditional local mutation operators with structural ones to transform it into a set of passing test programs. Since not all the passing test programs can help isolate compiler bugs effectively, RecBi further leverages reinforcement learning to intelligently guide the process of passing test program generation. Then, RecBi ranks all the suspicious files by analyzing the compiler execution traces of the generated passing test programs and the given failing test program following the practice of compiler bug isolation. The experimental results on 120 real bugs from two most popular C open-source compilers, i.e., GCC and LLVM, show that RecBi is able to isolate about 23\%/58\%/78\% bugs within Top-l/Top-5/Top-10 compiler files, and significantly outperforms the state-of-the-art compiler bug isolation approach by improving 92.86\%/55.56\%/25.68\% isolation effectiveness in terms of Top-l/Top-5/Top-10 results.		Included	new_screen		4
RL4SE	Chen2019b	Relational Verification using Reinforcement Learning	Relational verification aims to prove properties that relate a pair of programs or two different runs of the same program. While relational properties (e.g., equivalence, non-interference) can be verified by reducing them to standard safety, there are typically many possible reduction strategies, only some of which result in successful automated verification. Motivated by this problem, we propose a new relational verification algorithm that learns useful reduction strategies using reinforcement learning. Specifically, we show how to formulate relational verification as a Markov decision process (MDP) and use reinforcement learning to synthesize an optimal policy for the underlying MDP. The learned policy is then used to guide the search for a successful verification strategy. We have implemented this approach in a tool called COEUS and evaluate it on two benchmark suites. Our evaluation shows that COEUS solves significantly more problems within a given time limit compared to multiple baselines, including two state-of-the-art relational verification tools.	https://dx.doi.org/10.1145/3360567	Included	conflict_resolution		4
RL4SE	Chen2022c	Gait Parameter Optimization of Quadruped Robot Under Energy Consumption Index Based on Reinforcement Learning	In this paper, the gait optimization of quadruped bionic robot is studied under the unified energy consumption index. Firstly, an energy consumption index of quadruped robot is established. Secondly, the reinforcement learning method is used to optimize the gait parameters of the quadruped robot, so that the quadruped robot can gradually find the gait parameter combination with the lowest energy consumption in the current state in the interaction with the environment. In order to verify the effectiveness of this method, this paper completes the optimization of gait parameters combined with MIT cheetah software and DDQN network.	https://dx.doi.org/10.1109/ICARM54641.2022.9959165	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021c	Urban Parking Scheme in Hangzhou Based on Reinforcement Learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101723706&doi=10.1088\%2f1755-1315\%2f638\%2f1\%2f012002&partnerID=40&md5=548cf506f8f47341509f4f3b8ee7fe9e	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021d	Software-in-the-Loop Combined Reinforcement Learning Method for Dynamic Response Analysis of FOWTs	Floating offshore wind turbines (FOWTs) still face many challenges on how to better predict the dynamic responses. Artificial intelligence (AI) brings a new solution to overcome these challenges with intelligent strategies. A new AI technology-based method, named SADA, is proposed in this paper for the prediction of dynamic responses of FOWTs. Firstly, the methodology of SADA is introduced with the selection of Key Disciplinary Parameters (KDPs). The AI module in SADA was built in a coupled aero-hydro-servo-elastic in-house program DARwind and the policy decision is provided by the machine learning algorithms deep deterministic policy gradient (DDPG). Secondly, a set of basin experimental results of a Hywind Spar-type FOWT were employed to train the AI module. SADA weights KDPs by DDPG algorithms' actor network and changes their values according to the training feedback of 6DOF motions of Hywind platform through comparing the DARwind simulation results and that of experimental data. Many other dynamic responses that cannot be measured in basin experiment could be predicted in higher accuracy with this intelligent DARwind. Finally, the case study of SADA method was conducted and the results demonstrated that the mean values of the platform's motions can be predicted by AI-based DARwind with higher accuracy, for example the maximum error of surge motion is reduced by 21\%. This proposed SADA method takes advantage of numerical-experimental method and the machine learning method, which brings a new and promising solution for overcoming the handicap impeding direct use of traditional basin experimental technology in FOWTs design.	https://dx.doi.org/10.3389/fmars.2020.628225	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021e	Software-in-the-Loop Combined Machine Learning for Dynamic Responses Analysis of Floating Offshore Wind Turbines	Artificial intelligence (AI) brings a new solution to overcome the challenges of Floating offshore wind turbines (FOWTs) to better predict the dynamic responses with intelligent strategies. A new AI-based software-in-the-loop method, named SADA is introduced in this paper for the prediction of dynamic responses of FOWTs, which is proposed based on an in-house programme DARwind. DARwind is a coupled aero-hydro-servo-elastic in-house program for FOWTs, and a reinforcement learning method with exhaust algorithm and deep deterministic policy gradient (DDPG) are embedded in DARwind as an AI module. Firstly, the methodology is introduced with the selection of Key Disciplinary Parameters (KDPs). Secondly, Brute-force Method and DDPG algorithms are adopted to changes the KDPs' values according to the feedback of 6DOF motions of Hywind Spar-type platform through comparing the DARwind simulation results and those of basin experimental data. Therefore, many other dynamic responses that cannot be measured in basin experiment can be predicted in good accuracy with SADA method. Finally, the case study of SADA method was conducted and the results demonstrated that the mean values of the platform's motions can be predicted with higher accuracy. This proposed SADA method takes advantage of numerical-experimental method, basin experimental data and the machine learning technology, which brings a new and promising solution for overcoming the handicap impeding direct use of conventional basin experimental way to analyze FOWT's dynamic responses during the design phase.		Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2019c	An Adaptive Control Method for Arterial Signal Coordination Based on Deep Reinforcement Learning	Traffic signal control is a complex problem and it is difficult to determine an optimal strategy to control multi-directional traffic at multiple intersections. Recent years have witnessed numerous successes of deep learning neural networks in the fields of artificial intelligence. Motivated by the dominant performance of neural networks, this study attempts to develop a novel adaptive signal control approach by fusing deep learning (DL) and reinforcement learning (RL), i.e., deep reinforcement learning (DRL), for arterial signal coordination. DRL can considerably improve the ability to deal with large amounts of data processing, systematic perception and expression, which is key to coordinated control of arterial intersections. The proposed algorithm is implemented by utilizing real-time traffic detection data and aims to optimize the hybrid global and local reward functions. The experimental results obtained by traffic simulation software SUMO demonstrate the advantage of the proposed approach, as well as its efficiency and effectiveness compared with fixed-time and actual signal control methods.	https://dx.doi.org/10.1109/ITSC.2019.8917051	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2019d	An Adaptive Control Method for Arterial Signal Coordination Based on Deep Reinforcement Learning<sup>*</sup>		https://doi.org/10.1109/ITSC.2019.8917051	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2011	On optimizing vehicular dynamic spectrum access networks: Automation and learning in mobile wireless environments	In this paper, we propose a novel architecture for optimizing the overall performance of vehicular dynamic spectrum access (VDSA) networks. Due to the high level of mobility for vehicles operating under highway conditions, coupled with spatially variant spectrum allocation across a large geographical region, we envision that future vehicular communications will employ a form of dynamic spectrum access (DSA) in order to facilitate wireless transmissions between vehicles and with roadside infrastructure. In particular, the VDSA concept will be enabled by a combination of software-defined radio (SDR) technology, spectral occupancy databases, and machine learning techniques for enabling network automation. A vehicular networking scenario is substantially different relative to a generic mobile scenario with respect to the high level of mobility involved, the predictable trajectories of the vehicular traffic, and the overall scale of the network range. Consequently, the proposed architecture is designed to enable VDSA in a more flexible wireless spectrum environment by leveraging the cognitive radio concept and existing wireless spectrum databases actively being developed while simultaneously being compatible with current spectrum regulations. Regarding practical issues for vehicular communications, vehicle mobility is taken into account in order to ensure primary user protection, databases and channel priority schemes are used in order to record temporal and spatial channel heterogeneity, and vehicle path prediction techniques are employed in order to enhance channel access in this operating environment. Specifically, we show the advantages of employing the proposed learning architecture via a case study where reinforcement learning is used in order to achieve intelligent channel selection within a realistic VDSA environment. Moreover, performance enhancements in terms of channel switching times, interference, and throughput are shown via computer simulations.	https://dx.doi.org/10.1109/VNC.2011.6117122	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021f	User Preference-Based Demand Response for Smart Home Energy Management Using Multiobjective Reinforcement Learning	A well-designed demand response (DR) program is essential in smart home to optimize energy usage according to user preferences. In this study, we proposed a multiobjective reinforcement learning (MORL) algorithm to design a DR program. The proposed approach improved conventional algorithms by mitigating the effect of the change in user preferences and addressed the uncertainty induced by future price and renewable energy generation. Because two Q-tables were used, the proposed algorithm simultaneously considers electricity cost and user dissatisfaction; when user preference changes, the proposed MORL algorithm uses the previous experience to customize appliances' scheduling and swiftly achieve the best objective value. The generalizability of the proposed algorithm is high. Therefore, the algorithm can be implemented in a smart home equipped with an energy storage system, renewable energy source, and various types of appliances such as inflexible, time-flexible, and power-flexible ones. Numerical analysis using real-world data revealed that in case of price and renewable uncertainty, the proposed approach can deliver excellent performance after a change of user preference; it achieved 8.44\% cost reduction as compared with mixed-integer nonlinear programming based DR while increasing the dissatisfaction level only by 1.37\% on average.	https://dx.doi.org/10.1109/ACCESS.2021.3132962	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2021g	Optimal demand response strategy of commercial building-based virtual power plant using reinforcement learning	In this paper, the optimal demand response strategy of a commercial building-based virtual power plant with real-world implementation in heavily urbanised area is studied. Instead of modelling the decision-making process as an optimisation problem, a reinforcement learning method is used to seek the optimal strategy, which could update its performance with minimal manpower manipulation. Specifically, the data collection from several commercial buildings, including hotel, shopping mall and office, in Huangpu district, Shanghai city is analysed to deploy the demand response program. Compared with the conventional demand response strategy based on optimisation, the learnt strategy does not rely on the forecasting information as input and could adapt to the changing demand response incentive automatically. It may not produce the best result every time, but can guarantee the benefit in a non-deterministic way in long-term operation. The real-world deployment of the Huangpu virtual power plant involving hardware and software platform is also introduced, as well as its future development projection.	https://dx.doi.org/10.1049/gtd2.12179	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2013	Ketamine use among regular tobacco and alcohol users as revealed by respondent-driven sampling in Taipei: Prevalence, expectancy, and users' risky decision making	The popularity of ketamine for recreational use among young people began to increase, particularly in Asia, in 2000. To gain more knowledge about the use of ketamine among high-risk individuals, a respondent-driven sampling (RDS) was implemented among regular alcohol and tobacco users in the Taipei metropolitan area from 2007 to 2010. The sampling was initiated in three different settings (i.e., 2 in the community and 1 in a clinic) to recruit seed individuals. Each participant was asked to refer one to five friends known to be regular tobacco smokers and alcohol drinkers to participate in the present study. Incentives were offered differentially upon the completion of an interview and successful referral. Information pertaining to drug use experience was collected by an audio computer-assisted self-interview instrument. Software built for RDS analyses was used for data analyses. Of the 1,115 participants recruited, about 11.7\% of the RDS respondents reported ever having used ketamine. Positive expectancy of ketamine use was positively associated with ketamine use; by contrast, negative expectancy was inversely associated with ketamine use. Decision-making characteristics as measured on the Iowa Gambling Task (IGT) using reinforcement learning models revealed that ketamine users learned less from the most recent event than both tobacco- and drug-naive controls and regular tobacco and alcohol users. These findings about ketamine use among young people have implications for its prevention and intervention. Copyright (C) 2013, Food and Drug Administration, Taiwan. Published by Elsevier Taiwan LLC. All rights reserved.	https://www.ncbi.nlm.nih.gov/pubmed/25264412	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E1: Does not define or use a RL method	4
RL4SE	Chen2018b	Reinforcement learningendashbased QoS/QoE-aware service function chaining in software-driven 5G slices		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054060336&doi=10.1002\%2fett.3477&partnerID=40&md5=b832b11a85d46c908ca5658dbf093a00	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018c	Reinforcement learningendashbased QoS/QoE?aware service function chaining in software?driven 5G slices		https://doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018d	Reinforcement learning-based QoS/QoE-aware service function chaining in software-driven 5G slices	With the ever-growing diversity of devices and applications that will be connected to 5G networks, flexible and agile service orchestration with acknowledged quality of experience (QoE) that satisfies the end user's functional and quality-of-service (QoS) requirements is necessary. Software-defined networking (SDN) and network function virtualization (NFV) are considered key enabling technologies for 5G core networks. In this regard, this paper proposes a reinforcement learning-based QoS/QoE-aware service function chaining (SFC) scheme in SDN/NFV-enabled 5G slices. First, it implements a lightweight QoS information collector based on the Link Layer Discovery Protocol, which works in a piggyback fashion on the southbound interface of the SDN controller, to enable QoS-awareness. Then, a deep Q-network-based orchestration agent is designed to support SFC in the context of NFV. The agent takes into account the QoE and QoS as key aspects to formulate the reward so that it is expected to maximize QoE while respecting QoS constraints. The experiment results show that the proposed framework exhibits good performance in QoE provisioning and QoS requirements maintenance for SFC in dynamic network environments.	https://dx.doi.org/10.1002/ett.3477	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018e	Towards synthesizing complex programs from input-output examples			Included	new_screen		4
RL4SE	Chen2018f	Human-like longitudinal velocity control based on continuous reinforcement learning		https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044201074&doi=10.1061\%2f9780784480915.100&partnerID=40&md5=d58324d107f48b0ab009ebb1e7a67b97	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022d	Resource Allocation with Workload-Time Windows for Cloud-Based Software Services: A Deep Reinforcement Learning Approach	As the workloads and service requests in cloud computing environments change constantly, cloud-based software services need to adaptively allocate resources for ensuring the Quality-of-Service (QoS) while reducing resource costs. However, it is very challenging to achieve adaptive resource allocation for cloud-based software services with complex and variable system states. Most of the existing methods only consider the current condition of workloads, and thus cannot well adapt to real-world cloud environments subject to fluctuating workloads. To address this challenge, we propose a novel Deep Reinforcement learning based resource Allocation method with workload-time Windows (DRAW) for cloud-based software services that considers both the current and future workloads in the resource allocation process. Specifically, an original Deep Q-Network (DQN) based prediction model of management operations is trained based on workload-time windows, which can be used to predict appropriate management operations under different system states. Next, a new feedback-control mechanism is designed to construct the objective resource allocation plan under the current system state through iterative execution of management operations. Extensive simulation results demonstrate that the prediction accuracy of management operations generated by the proposed DRAW method can reach 90.69\%. Moreover, the DRAW can achieve the optimal/near-optimal performance and outperform other classic methods by 3~13\% under different scenarios.	https://dx.doi.org/10.1109/TCC.2022.3169157	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022e	Study on numerical simulation of complex wave based on deep reinforcement learning method			Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2022f	Resource Allocation for Cloud-Based Software Services Using Prediction-Enabled Feedback Control With Reinforcement Learning	With time-varying workloads and service requests, cloud-based software services necessitate adaptive resource allocation for guaranteeing Quality-of-Service (QoS) and reducing resource costs. However, due to the ever-changing system states, resource allocation for cloud-based software services faces huge challenges in dynamics and complexity. The traditional approaches mostly rely on expert knowledge or numerous iterations, which might lead to weak adaptiveness and extra costs. Moreover, existing RL-based methods target the environment with the fixed workload, and thus they are unable to effectively fit in the real-world scenarios with variable workloads. To address these important challenges, we propose a Prediction-enabled feedback Control with Reinforcement learning based resource Allocation (PCRA) method. First, a novel Q-value prediction model is designed to predict the values of management operations (by Q-values) at different system states. The model uses multiple prediction learners for making accurate Q-value prediction by integrating the Q-learning algorithm. Next, the objective resource allocation plans can be found by using a new feedback-control based decision-making algorithm. Using the RUBiS benchmark, simulation results demonstrate that the PCRA chooses the management operations of resource allocation with 93.7 percent correctness. Moreover, the PCRA achieves optimal/near-optimal performance, and it outperforms the classic ML-based and rule-based methods by 5$\sim$?7\% and 10$\sim$?13\%, respectively.	https://dx.doi.org/10.1109/TCC.2020.2992537	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2018g	DQN-Based Power Control for IoT Transmission against Jamming	Internet of Things (IoTs) have to address jammers, with goal to interrupt the communication of the energy- constrained IoT devices and sometimes even cause denial-of-service attacks. In this paper, we propose a deep reinforcement learning based power control scheme for IoT devices to improve the transmission efficiency and save energy. This scheme depends on the current IoT transmission status and the jamming strength and applies deep Q-network (DQN) to determine the transmit power without being aware of the IoT topology and the jamming model. This scheme is implemented on the universal software radio peripherals for the anti- jamming communication performance evaluation. Experimental results show that this scheme improves the signal-to-interference-plus-noise of the IoT signals compared with the benchmark Q-learning based power control scheme against jamming.	https://dx.doi.org/10.1109/VTCSpring.2018.8417695	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
RL4SE	Chen2007	Genetic network programming with sarsa learning and its application to creating stock trading rules		https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749086694&doi=10.1109\%2fCEC.2007.4424475&partnerID=40&md5=4c30d9a4fb58ed219a3a53bb50f142ab	Excluded	new_screen	E2: Software engineering is not the problem RL is used for,E2: Software engineering is not the problem RL is used for	4
